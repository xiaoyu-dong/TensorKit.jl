<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tensors and the TensorMap type ¬∑ TensorKit.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">TensorKit.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../intro/">Introduction</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../categories/">Optional introduction to category theory</a></li><li><a class="tocitem" href="../spaces/">Vector spaces</a></li><li><a class="tocitem" href="../sectors/">Sectors, representation spaces and fusion trees</a></li><li class="is-active"><a class="tocitem" href>Tensors and the <code>TensorMap</code> type</a><ul class="internal"><li><a class="tocitem" href="#ss_tensor_storage-1"><span>Storage of tensor data</span></a></li><li><a class="tocitem" href="#ss_tensor_construction-1"><span>Constructing tensor maps and accessing tensor data</span></a></li><li><a class="tocitem" href="#ss_tensor_properties-1"><span>Tensor properties</span></a></li><li><a class="tocitem" href="#ss_tensor_linalg-1"><span>Vector space and linear algebra operations</span></a></li><li><a class="tocitem" href="#Index-manipulations-1"><span>Index manipulations</span></a></li><li><a class="tocitem" href="#Tensor-factorizations-1"><span>Tensor factorizations</span></a></li><li><a class="tocitem" href="#Bosonic-tensor-contractions-and-tensor-networks-1"><span>Bosonic tensor contractions and tensor networks</span></a></li><li><a class="tocitem" href="#Fermionic-tensor-contractions-1"><span>Fermionic tensor contractions</span></a></li><li><a class="tocitem" href="#Anyonic-tensor-contractions-1"><span>Anyonic tensor contractions</span></a></li></ul></li></ul></li><li><span class="tocitem">Library</span><ul><li><a class="tocitem" href="../../lib/spaces/">Vector spaces, symmetry sectors an fusion trees</a></li></ul></li><li><span class="tocitem">Index</span><ul><li><a class="tocitem" href="../../index/">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Tensors and the <code>TensorMap</code> type</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tensors and the <code>TensorMap</code> type</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/Jutho/TensorKit.jl/blob/master/docs/src/man/tensors.md" title="Edit on GitHub"><span class="docs-icon fab">ÔÇõ</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="s_tensors-1"><a class="docs-heading-anchor" href="#s_tensors-1">Tensors and the <code>TensorMap</code> type</a><a class="docs-heading-anchor-permalink" href="#s_tensors-1" title="Permalink"></a></h1><p>This last page explains how to create and manipulate tensors in TensorKit.jl. As this is probably the most important part of the manual, we will also focus more strongly on the usage and interface, and less so on the underlying implementation. The only aspect of the implementation that we will address is the storage of the tensor data, as this is important to know how to create and initialize a tensor, but will in fact also shed light on how some of the methods work.</p><p>As mentioned, all tensors in TensorKit.jl are interpreted as linear maps (morphisms) from a domain (a <code>ProductSpace{S,N‚ÇÇ}</code>) to a domain (another <code>ProductSpace{S,N‚ÇÅ}</code>), with the same <code>S&lt;:ElementarySpace</code> that labels the type of spaces associated with the individual tensor indices. The overall type for all such tensor maps is <code>AbstractTensorMap{S, N‚ÇÅ, N‚ÇÇ}</code>. Note that we place information about the codomain before that of the domain. Indeed, we have already encountered the constructor for the concrete parametric type <code>TensorMap</code> in the form <code>TensorMap(..., codomain, domain)</code>. This convention is opposite to the mathematical notation, e.g. <span>$\mathrm{Hom}(W,V)$</span> or <span>$f:W‚ÜíV$</span>, but originates from the fact that a normal matrix is also denoted as having size <code>m √ó n</code> or is constructed in Julia as <code>Array(..., (m, n))</code>, where the first integer <code>m</code> refers to the codomain being <code>m</code>- dimensional, and the seond integer <code>n</code> to the domain being <code>n</code>-dimensional. This also explains why we have consistently used the symbol <span>$W$</span> for spaces in the domain and <span>$V$</span> for spaces in the codomain. A tensor map <span>$t:(W‚ÇÅ ‚äó ‚Ä¶ ‚äó W_{N‚ÇÇ}) ‚Üí (V‚ÇÅ ‚äó ‚Ä¶ ‚äó V_{N‚ÇÅ})$</span> will be created in Julia as <code>TensorMap(..., V1 ‚äó ... ‚äó VN‚ÇÅ, W1 ‚äó ... ‚äó WN2)</code>.</p><p>Furthermore, the abstract type <code>AbstractTensor{S,N}</code> is just a synonym for <code>AbstractTensorMap{S,N,0}</code>, i.e. for tensor maps with an empty domain, which is equivalent to the unit of the monoidal category, or thus, the field of scalars <span>$ùïú$</span>.</p><p>Currently, <code>AbstractTensorMap</code> has two subtypes. <code>TensorMap</code> provides the actual implementation, where the data of the tensor is stored in a <code>DenseArray</code> (more specifically a <code>DenseMatrix</code> as will be explained below). <code>AdjointTensorMap</code> is a simple wrapper type to denote the adjoint of an existing <code>TensorMap</code> object. In the future, additional types could be defined, to deal with sparse data, static data, diagonal data, etc...</p><h2 id="ss_tensor_storage-1"><a class="docs-heading-anchor" href="#ss_tensor_storage-1">Storage of tensor data</a><a class="docs-heading-anchor-permalink" href="#ss_tensor_storage-1" title="Permalink"></a></h2><p>Before discussion how to construct and initalize a <code>TensorMap{S}</code>, let us discuss what is meant by &#39;tensor data&#39; and how it can efficiently and compactly be stored. Let us first discuss the case <code>sectortype(S) == Trivial</code> sector, i.e. the case of no symmetries. In that case the data of a tensor <code>t = TensorMap(..., V1 ‚äó ... ‚äó VN‚ÇÅ, W1 ‚äó ... ‚äó WN2)</code> can just be represented as a multidimensional array of size</p><p><code>(dim(V1), dim(V2), ‚Ä¶, dim(VN‚ÇÅ), dim(W1), ‚Ä¶, dim(WN‚ÇÇ))</code></p><p>which can also be reshaped into matrix of size</p><p><code>(dim(V1)*dim(V2)*‚Ä¶*dim(VN‚ÇÅ), dim(W1)*dim(W2)*‚Ä¶*dim(WN‚ÇÇ))</code></p><p>and is really the matrix representation of the linear map that the tensor represents. In particular, given a second tensor <code>t2</code> whose domain matches with the codomain of <code>t</code>, function composition amounts to multiplication of their corresponding data matrices. Similarly, tensor factorizations such as the singular value decomposition, which we discuss below, can act directly on this matrix representation.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>One might wonder if it would not have been more natural to represent the tensor data as <code>(dim(V1), dim(V2), ‚Ä¶, dim(VN‚ÇÅ), dim(WN‚ÇÇ), ‚Ä¶, dim(W1))</code> given how employing the duality naturally reverses the tensor product, as encountered with the interface of <a href="man/@ref"><code>repartition</code></a> for <a href="../sectors/#ss_fusiontrees-1">fusion trees</a>. However, such a representation, when plainly <code>reshape</code>d to a matrix, would not have the above properties and would thus not constitute the matrix representation of the tensor in a compatible basis.</p></div></div><p>Now consider the case where <code>sectortype(S) == G</code> for some <code>G</code> which has <code>FusionStyle(G) == Abelian()</code>, i.e. the representations of an Abelian group, e.g. <code>G == ‚Ñ§‚ÇÇ</code> or <code>G == U‚ÇÅ</code>. In this case, the tensor data is associated with sectors <code>(a1, a2, ‚Ä¶, aN‚ÇÅ) ‚àà sectors(V1 ‚äó V2 ‚äó ‚Ä¶ ‚äó VN‚ÇÅ)</code> and <code>(b1, ‚Ä¶, bN‚ÇÇ) ‚àà sectors(W1 ‚äó ‚Ä¶ ‚äó WN‚ÇÇ)</code> such that they fuse to a same common charge, i.e. <code>(c = first(‚äó(a1, ‚Ä¶, aN‚ÇÅ))) == first(‚äó(b1, ‚Ä¶, bN‚ÇÇ))</code>. The data associated with this takes the form of a multidimensional array with size <code>(dim(V1, a1), ‚Ä¶, dim(VN‚ÇÅ, aN‚ÇÅ), dim(W1, b1), ‚Ä¶, dim(WN‚ÇÇ, bN‚ÇÇ))</code>, or equivalently, a matrix of with row size <code>dim(V1, a1)*‚Ä¶*dim(VN‚ÇÅ, aN‚ÇÅ) == dim(codomain, (a1, ‚Ä¶, aN‚ÇÅ))</code> and column size <code>dim(W1, b1)*‚Ä¶*dim(WN‚ÇÇ, aN‚ÇÇ) == dim(domain, (b1, ‚Ä¶, bN‚ÇÇ))</code>.</p><p>However, there are multiple combinations of <code>(a1, ‚Ä¶, aN‚ÇÅ)</code> giving rise to the same <code>c</code>, and so there is data associated with all of these, as well as all possible combinations of <code>(b1, ‚Ä¶, bN‚ÇÇ)</code>. Stacking all matrices for different <code>(a1,‚Ä¶)</code> and a fixed value of <code>(b1,‚Ä¶)</code> underneath each other, and for fixed value of <code>(a1,‚Ä¶)</code> and different values of <code>(b1,‚Ä¶)</code> next to each other, gives rise to a larger block matrix of all data associated with the central sector <code>c</code>. The size of this matrix is exactly <code>(blockdim(codomain, c), blockdim(domain, c))</code> and these matrices are exactly the diagonal blocks whose existence is guaranteed by Schur&#39;s lemma, and which are labeled by the coupled sector <code>c</code>. Indeed, if we would represent the tensor map <code>t</code> as a matrix without explicitly using the symmetries, we could reorder the rows and columns to group data corresponding to sectors that fuse to the same <code>c</code>, and the resulting block diagonal representation would emerge. This basis transform is thus a permutation, which is a unitary operation, that will cancel or go through trivially for linear algebra operations such as composing tensor maps (matrix multiplication) or tensor factorizations such as a singular value decomposition. For such linear algebra operations, we can thus directly act on these large matrices, which correspond to the diagonal blocks that emerge after a basis transform, provided that the partition of the tensor indices in domain and codomain of the tensor are in line with our needs. For example, composing two tensor maps amounts to multiplying the matrices corresponding to the same <code>c</code> (provided that its subblocks labeled by the different combinations of sectors are ordered in the same way, which we guarantee by associating a canonical order with sectors). Henceforth, we refer to the <code>blocks</code> of a tensor map as those diagonal blocks, the existence of which is provided by Schur&#39;s lemma and which are labeled by the coupled sectors <code>c</code>. We directly store these blocks as <code>DenseMatrix</code> and gather them as values in a dictionary, together with the corresponding coupled sector <code>c</code> as key. For a given tensor <code>t</code>, we can access a specific block as <code>block(t, c)</code>, whereas <code>blocks(t)</code> yields an iterator over pairs <code>c=&gt;block(t,c)</code>.</p><p>The subblocks corresponding to a particular combination of sectors then correspond to a particular view for some range of the rows and some range of the colums, i.e. <code>view(block(t, c), m‚ÇÅ:m‚ÇÇ, n‚ÇÅ:n‚ÇÇ)</code> where the ranges <code>m‚ÇÅ:m‚ÇÇ</code> associated with <code>(a1, ‚Ä¶, aN‚ÇÅ)</code> and <code>n‚ÇÅ:n‚ÇÇ</code> associated with <code>(b‚ÇÅ, ‚Ä¶, bN‚ÇÇ)</code> are stored within the fields of the instance <code>t</code> of type <code>TensorMap</code>. This <code>view</code> can then lazily be reshaped to a multidimensional array, for which we rely on the package <a href="https://github.com/Jutho/Strided.jl">Strided.jl</a>. Indeed, the data in this <code>view</code> is not contiguous, because the stride between the different columns is larger than the length of the columns. Nonetheless, this does not pose a problem and even as multidimensional array there is still a definite stride associated with each dimension.</p><p>When <code>FusionStyle(G) isa NonAbelian</code>, things become slightly more complicated. Not only do <code>(a1, ‚Ä¶, aN‚ÇÅ)</code> give rise to different coupled sectors <code>c</code>, there can be multiply ways in which they fuse to <code>c</code>. These different possibilities are enumerated by the iterator <code>fusiontrees((a1, ‚Ä¶, aN‚ÇÅ), c)</code> and <code>fusiontrees((b1, ‚Ä¶, bN‚ÇÇ), c)</code>, and with each of those, there is tensor data that takes the form of a multidimensional array, or, after reshaping, a matrix of size <code>(dim(codomain, (a1, ‚Ä¶, aN‚ÇÅ)), dim(domain, (b1, ‚Ä¶, bN‚ÇÇ))))</code>. Again, we can stack all such matrices with the same value of <code>f‚ÇÅ ‚àà fusiontrees((a1, ‚Ä¶, aN‚ÇÅ), c)</code> horizontally (as they all have the same number of rows), and with the same value of <code>f‚ÇÇ ‚àà fusiontrees((b1, ‚Ä¶, bN‚ÇÇ), c)</code> vertically (as they have the same number of columns). What emerges is a large matrix of size <code>(blockdim(codomain, c), blockdim(domain, c))</code> containing all the tensor data associated with the coupled sector <code>c</code>, where <code>blockdim(P, c) = sum(dim(P, s)*length(fusiontrees(s, c)) for s in sectors(P))</code> for some instance <code>P</code> of <code>ProductSpace</code>. The tensor implementation does not distinguish between abelian or non-abelian sectors and still stores these matrices as a <code>DenseMatrix</code>, accessible via <code>block(t, c)</code>.</p><p>At first sight, it might now be less clear what the relevance of this block is in relation to the full matrix representation of the tensor map, where the symmetry is not exploited. The essential interpretation is still the same. Schur&#39;s lemma now tells that there is a unitary basis transform which makes this matrix representation block diagonal, more specifically, of the form <span>$‚®Å_{c} B_c ‚äó ùüô_{c}$</span>, where <span>$B_c$</span> denotes <code>block(t,c)</code> and <span>$ùüô_{c}$</span> is an identity matrix of size <code>(dim(c), dim(c))</code>. The reason for this extra identity is that the group representation is recoupled to act as <span>$‚®Å_{c} ùüô ‚äó u_c(g)$</span> for all <span>$g ‚àà \mathsf{G}$</span>, with <span>$u_c(g)$</span> the matrix representation of group element <span>$g$</span> according to the irrep <span>$c$</span>. In the abelian case, <code>dim(c) == 1</code>, i.e. all irreducible representations are one-dimensional and Schur&#39;s lemma only dictates that all off-diagonal blocks are zero. However, in this case the basis transform to the block diagonal representation is not simply a permutation matrix, but a more general unitary matrix composed of the different fusion trees. Indeed, let us denote the fusion trees <code>f‚ÇÅ ‚àà fusiontrees((a1, ‚Ä¶, aN‚ÇÅ), c)</code> as <span>$X^{a_1, ‚Ä¶, a_{N‚ÇÅ}}_{c,Œ±}$</span> where <span>$Œ± = (e_1, ‚Ä¶, e_{N_1-2}; Œº‚ÇÅ, ‚Ä¶, Œº_{N_1-1})$</span> is a collective label for the internal sectors <code>e</code> and the vertex degeneracy labels <code>Œº</code> of a generic fusion tree, as discussed in the <a href="../sectors/#ss_fusiontrees-1">corresponding section</a>. The tensor is then represented as</p><p><img src="../img/tensor-storage.svg" alt="tensor storage"/></p><p>In this diagram, we have indicated how the tensor map can be rewritten in terms of a block diagonal matrix with a unitary matrix on its left and another unitary matrix (if domain and codomain are different) on its right. So the left and right matrices should actually have been drawn as squares. They represent the unitary basis transform. In this picture, red and white regions are zero. The center matrix is most easy to interpret. It is the block diagonal matrix <span>$‚®Å_{c} B_c ‚äó ùüô_{c}$</span> with diagonal blocks labeled by the coupled charge <code>c</code>, in this case it takes two values. Every single small square in between the dotted or dashed lines has size <span>$d_c √ó d_c$</span> and corresponds to a single element of <span>$B_c$</span>, tensored with the identity <span>$\mathrm{id}_c$</span>. Instead of <span>$B_c$</span>, a more accurate labelling is <span>$t^c_{(a_1 ‚Ä¶ a_{N‚ÇÅ})Œ±, (b_1 ‚Ä¶ b_{N‚ÇÇ})Œ≤}$</span> where <span>$Œ±$</span> labels different fusion trees from <span>$(a_1 ‚Ä¶ a_{N‚ÇÅ})$</span> to <span>$c$</span>. The dashed horizontal lines indicate regions corresponding to different fusion (actually splitting) trees, either because of different sectors <span>$(a_1 ‚Ä¶ a_{N‚ÇÅ})$</span> or different labels <span>$Œ±$</span> within the same sector. Similarly, the dashed vertical lines define the border between regions of different fusion trees from the domain to <code>c</code>, either because of different sectors <span>$(b_1 ‚Ä¶ b_{N‚ÇÇ})$</span> or a different label <span>$Œ≤$</span>.</p><p>To understand this better, we need to understand the basis transform, e.g. on the left (codomain) side. In more detail, it is given by</p><p><img src="../img/tensor-unitary.svg" alt="tensor unitary"/></p><p>Indeed, remembering that <span>$V_i = ‚®Å_{a_i} R_{a_i} ‚äó ‚ÑÇ^{n_{a_i}}$</span> with <span>$R_{a_i}$</span> the representation space on which irrep <span>$a_i$</span> acts (with dimension <span>$\mathrm{dim}(a_i)$</span>), we find <span>$V_1 ‚äó ‚Ä¶ ‚äó V_{N_1} = ‚®Å_{a_1, ‚Ä¶, a_{N‚ÇÅ}} (R_{a_1} ‚äó ‚Ä¶ ‚äó R_{a_{N_1}}) ‚äó ‚ÑÇ^{n_{a_1} √ó ‚Ä¶ n_{a_{N_1}}}$</span>. In the diagram above, the wiggly lines correspond to the direct sum over the different sectors <span>$(a_1, ‚Ä¶, a_{N‚ÇÅ})$</span>, there depicted taking three possible values <span>$(a‚Ä¶)$</span>, <span>$(a‚Ä¶)‚Ä≤$</span> and <span>$(a‚Ä¶)‚Ä≤‚Ä≤$</span>. The tensor product <span>$(R_{a_1} ‚äó ‚Ä¶ ‚äó R_{a_{N_1}}) ‚äó ‚ÑÇ^{n_{a_1} √ó ‚Ä¶ n_{a_{N_1}}}$</span> is depicted as <span>$(R_{a_1} ‚äó ‚Ä¶ ‚äó R_{a_{N_1}})^{‚äï n_{a_1} √ó ‚Ä¶ n_{a_{N_1}}}$</span>, i.e. as a direct sum of the spaces <span>$R_{(a‚Ä¶)} = (R_{a_1} ‚äó ‚Ä¶ ‚äó R_{a_{N_1}})$</span> according to the dotted horizontal lines, which repeat <span>$n_{(a‚Ä¶)} = n_{a_1} √ó ‚Ä¶ n_{a_{N_1}}$</span> times. In this particular example, <span>$n_{(a‚Ä¶)}=2$</span>, <span>$n_{(a‚Ä¶)&#39;}=3$</span> and <span>$n_{(a‚Ä¶)&#39;&#39;}=5$</span>. The thick vertical line represents the separation between the two different coupled sectors, denoted as <span>$c$</span> and <span>$c&#39;$</span>. Dashed vertical lines represent different ways of reaching the coupled sector, corresponding to different <code>Œ±</code>. In this example, the first sector <span>$(a‚Ä¶)$</span> has one fusion tree to <span>$c$</span>, labeled by <span>$c,Œ±$</span>, and two fusion trees to <span>$c&#39;$</span>, labeled by <span>$c&#39;,Œ±$</span> and <span>$c&#39;,Œ±&#39;$</span>. The second sector has only a fusion tree to <span>$c$</span>, labeled by <span>$c,Œ±&#39;$</span>. The third sector only has a fusion tree to <span>$c&#39;$</span>, labeld by <span>$c&#39;, Œ±&#39;&#39;$</span>. Finally then, because the fusion trees do not act on the spaces <span>$‚ÑÇ^{n_{a_1} √ó ‚Ä¶ n_{a_{N_1}}}$</span>, the dotted lines which represent the different <span>$n_{(a‚Ä¶)} = n_{a_1} √ó ‚Ä¶ n_{a_{N_1}}$</span> dimensions are also drawn vertically. In particular, for a given sector <span>$(a‚Ä¶)$</span> and a specific fusion tree <span>$X^{(a‚Ä¶)}_{c,Œ±}: R_{(a‚Ä¶)}‚ÜíR_c$</span>, the action is <span>$X^{(a‚Ä¶)}_{c,Œ±} ‚äó ùüô_{n_{(a‚Ä¶)}}$</span>, which corresponds to the diagonal green blocks in this drawing where the same matrix <span>$X^{(a‚Ä¶)}_{c,Œ±}$</span> (the fusion tree) is repeated along the diagonal. Note that the fusion tree is not a vector or single column, but a matrix with number of rows equal to <span>$\mathrm{dim}(R_{(a\ldots)}) = d_{a_1} d_{a_2} ‚Ä¶ d_{a_{N_1}}$</span> and number of columns equal to <span>$d_c$</span>. A similar interpretation can be given to the basis transform on the right, by taking its adjoint. In this particular example, it has two different combinations of sectors <span>$(b‚Ä¶)$</span> and <span>$(b‚Ä¶)&#39;$</span>, where both have a single fusion tree to <span>$c$</span> as well as to <span>$c&#39;$</span>, and <span>$n_{(b‚Ä¶)}=2$</span>, <span>$n_{(b‚Ä¶)&#39;}=3$</span>.</p><p>Note that we never explicitly store or act with the basis transforms on the left and the right. For composing tensor maps (i.e. multiplying them), these basis transforms just cancel, whereas for tensor factorizations they just go through trivially. They transform non-trivially when reshuffling the tensor indices, both within or in between the domain and codomain. For this, however, we can completely rely on the manipulations of fusion trees to implicitly compute the effect of the basis transform and construct the new blocks <span>$B_c$</span> that result with respect to the new basis.</p><p>Hence, as before, we only store the diagonal blocks <span>$B_c$</span> of size <code>(blockdim(codomain(t), c), blockdim(domain(t), c))</code> as a <code>DenseMatrix</code>, accessible via <code>block(t, c)</code>. Within this matrix, there are regions of the form <code>view(block(t, c), m‚ÇÅ:m‚ÇÇ, n‚ÇÅ:n‚ÇÇ)</code> that correspond to the data <span>$t^c_{(a_1 ‚Ä¶ a_{N‚ÇÅ})Œ±, (b_1 ‚Ä¶ b_{N‚ÇÇ})Œ≤}$</span> associated with a pair of fusion trees <span>$X^{(a_1 ‚Ä¶ a_{N‚ÇÅ}}_{c,Œ±}$</span> and <span>$X^{(b_1 ‚Ä¶ b_{N‚ÇÇ})}_{c,Œ≤}$</span>, henceforth again denoted as <code>f‚ÇÅ</code> and <code>f‚ÇÇ</code>, with <code>f‚ÇÅ.coupled == f‚ÇÇ.coupled == c</code>. The ranges where this subblock is living are managed within the tensor implementation, and these subblocks can be accessed via <code>t[f‚ÇÅ,f‚ÇÇ]</code>, and is returned as a <code>StridedArray</code> of size <span>$n_{a_1} √ó n_{a_2} √ó ‚Ä¶ √ó n_{a_{N_1}} √ó n_{b_1} √ó ‚Ä¶ n_{b_{N‚ÇÇ}}$</span>, or in code, <code>(dim(V1, a1), dim(V2, a2), ‚Ä¶, dim(VN‚ÇÅ, aN‚ÇÅ), dim(W1, b1), ‚Ä¶, dim(WN‚ÇÇ, bN‚ÇÇ))</code>. While the implementation does not distinguish between <code>FusionStyle isa Abelian</code> or <code>FusionStyle isa NonAbelian</code>, in the former case the fusion tree is completely characterized by the uncoupled sectors, and so the subblocks can also be accessed as <code>t[(a1, ‚Ä¶, aN‚ÇÅ), (b1, ‚Ä¶, bN‚ÇÇ)]</code>. When there is no symmetry at all, i.e. <code>sectortype(t) == Trivial</code>, <code>t[]</code> returns the raw tensor data as a <code>StridedArray</code> of size <code>(dim(V1), ‚Ä¶, dim(VN‚ÇÅ), dim(W1), ‚Ä¶, dim(WN‚ÇÇ))</code>, whereas <code>block(t, Trivial())</code> returns the same data as a <code>DenseMatrix</code> of size <code>(dim(V1) * ‚Ä¶ * dim(VN‚ÇÅ), dim(W1) * ‚Ä¶ * dim(WN‚ÇÇ))</code>.</p><h2 id="ss_tensor_construction-1"><a class="docs-heading-anchor" href="#ss_tensor_construction-1">Constructing tensor maps and accessing tensor data</a><a class="docs-heading-anchor-permalink" href="#ss_tensor_construction-1" title="Permalink"></a></h2><p>Having learned how a tensor is represented and stored, we can now discuss how to create tensors and tensor maps. From hereon, we focus purely on the interface rather than the implementation.</p><p>The most convenient set of constructors are those that construct  tensors or tensor maps with random or uninitialized data. They take the form</p><p><code>TensorMap(f, codomain, domain)</code></p><p><code>TensorMap(f, eltype::Type{&lt;:Number}, codomain, domain)</code></p><p><code>TensorMap(undef, codomain, domain)</code></p><p><code>TensorMap(undef, eltype::Type{&lt;:Number}, codomain, domain)</code></p><p>Here, in the first form, <code>f</code> can be any function or object that is called with an argument of type <code>Dims{2} = Tuple{Int,Int}</code> and is such that <code>f((m,n))</code> creates a <code>DenseMatrix</code> instance with <code>size(f(m,n)) == (m,n)</code>. In the second form, <code>f</code> is called as <code>f(eltype,(m,n))</code>. Possibilities for <code>f</code> are <code>randn</code> and <code>rand</code> from Julia Base. TensorKit.jl provides <code>randnormal</code> and <code>randuniform</code> as an synonym for <code>randn</code> and <code>rand</code>, as well as the new function  <code>randisometry</code>, alternatively called <code>randhaar</code>, that creates a random isometric <code>m √ó n</code> matrix <code>w</code> satisfying <code>w&#39;*w ‚âà I</code> distributed according to the Haar measure (this requires <code>m&gt;= n</code>). The third and fourth calling syntax use the <code>UndefInitializer</code> from Julia Base and generates a <code>TensorMap</code> with unitialized data, which could thus contain <code>NaN</code>s.</p><p>In all of these constructors, the last two arguments can be replaced by <code>domain‚Üícodomain</code> or <code>codomain‚Üêdomain</code>, where the arrows are obtained as <code>\rightarrow+TAB</code> and <code>\leftarrow+TAB</code>. These arrows just create a Julia <code>Pair</code>, i.e. also <code>domain =&gt; codomain</code> can be used, provided that <code>domain</code> and <code>codomain</code> are of type <code>ProductSpace</code>. The advantage of the unicode arrows is that they will also convert a single instance of type <code>S&lt;:ElementarySpace</code> to a corresponding <code>ProductSpace{S,1}</code>. Some examples are perhaps in order</p><pre><code class="language-julia-repl">julia&gt; t1 = TensorMap(randnormal, ‚ÑÇ^2 ‚äó ‚ÑÇ^3, ‚ÑÇ^2)
TensorMap((‚ÑÇ^2 ‚äó ‚ÑÇ^3) ‚Üê ProductSpace(‚ÑÇ^2)):
[:, :, 1] =
 1.1571056827984751    0.2168890743280532   -1.1970305476018106
 0.07885765472696102  -0.16397115105684987  -0.4686437961292103

[:, :, 2] =
 0.37941246788618677  1.6641404317934416   0.2743806659529405
 1.0163585522540273   1.5669765541540777  -0.09698349099524221

julia&gt; t2 = TensorMap(randisometry, Float32, ‚ÑÇ^2 ‚äó ‚ÑÇ^3 ‚Üê ‚ÑÇ^2)
TensorMap((‚ÑÇ^2 ‚äó ‚ÑÇ^3) ‚Üê ProductSpace(‚ÑÇ^2)):
[:, :, 1] =
 -0.00541389f0   0.31333315f0  -0.8056578f0
 -0.41140822f0  -0.18064588f0  -0.22543041f0

[:, :, 2] =
  0.2838877f0    -0.31711748f0  -0.21437538f0
 -0.118112445f0  -0.3363316f0    0.803628f0

julia&gt; t3 = TensorMap(undef, ‚ÑÇ^2 ‚Üí ‚ÑÇ^2 ‚äó ‚ÑÇ^3)
TensorMap((‚ÑÇ^2 ‚äó ‚ÑÇ^3) ‚Üê ProductSpace(‚ÑÇ^2)):
[:, :, 1] =
 1.0e-323  5.0e-324  5.0e-324
 1.0e-323  5.0e-324  1.0e-323

[:, :, 2] =
 5.0e-324  5.0e-324  5.0e-324
 5.0e-324  5.0e-324  0.0

julia&gt; t4failed = TensorMap(undef, ComplexF64, ‚ÑÇ^2 =&gt; ‚ÑÇ^2 ‚äó ‚ÑÇ^3)
ERROR: MethodError: no method matching TensorMap(::UndefInitializer, ::Type{Complex{Float64}}, ::Pair{ComplexSpace,ProductSpace{ComplexSpace,2}})
Closest candidates are:
  TensorMap(::UndefInitializer, ::Type{T&lt;:Number}, !Matched::ProductSpace{S&lt;:ElementarySpace,N} where N, !Matched::ProductSpace{S&lt;:ElementarySpace,N} where N) where {S&lt;:ElementarySpace, T&lt;:Number} at /home/travis/build/Jutho/TensorKit.jl/src/tensors/tensor.jl:166
  TensorMap(::Any, ::Type{T&lt;:Number}, !Matched::ProductSpace{S&lt;:ElementarySpace,N} where N, !Matched::ProductSpace{S&lt;:ElementarySpace,N} where N) where {S&lt;:ElementarySpace, T&lt;:Number} at /home/travis/build/Jutho/TensorKit.jl/src/tensors/tensor.jl:159
  TensorMap(::Any, ::Type{T&lt;:Number}, !Matched::Union{ProductSpace{S&lt;:ElementarySpace,N} where N, S&lt;:ElementarySpace}, !Matched::Union{ProductSpace{S&lt;:ElementarySpace,N} where N, S&lt;:ElementarySpace}) where {T&lt;:Number, S&lt;:ElementarySpace} at /home/travis/build/Jutho/TensorKit.jl/src/tensors/tensor.jl:172
  ...

julia&gt; t4 = TensorMap(undef, ComplexF64, ProductSpace(‚ÑÇ^2) =&gt; ‚ÑÇ^2 ‚äó ‚ÑÇ^3)
TensorMap((‚ÑÇ^2 ‚äó ‚ÑÇ^3) ‚Üê ProductSpace(‚ÑÇ^2)):
[:, :, 1] =
  6.9197087291991e-310 + 6.91970873470455e-310im  ‚Ä¶  6.91970873466463e-310 + 6.91970873471324e-310im
 6.91970873458755e-310 + 6.919708734752e-310im       6.91970873459585e-310 + 6.9197087346437e-310im

[:, :, 2] =
 6.91970873447214e-310 + 6.9197087347251e-310im   ‚Ä¶  6.91970873295516e-310 + 6.91970873455e-310im
  6.9197087345421e-310 + 6.91970873472747e-310im     6.91970871561464e-310 + 6.9197087346097e-310im

julia&gt; domain(t1) == domain(t2) == domain(t3) == domain(t4)
true

julia&gt; codomain(t1) == codomain(t2) == codomain(t3) == codomain(t4)
true

julia&gt; disp(x) = show(IOContext(Core.stdout, :compact=&gt;false), &quot;text/plain&quot;, trunc.(x; digits = 3));

julia&gt; t1[] |&gt; disp
2√ó3√ó2 Strided.StridedView{Float64,3,Array{Float64,1},typeof(identity)}:
[:, :, 1] =
 1.157   0.216  -1.197
 0.078  -0.163  -0.468

[:, :, 2] =
 0.379  1.664   0.274
 1.016  1.566  -0.096
julia&gt; block(t1, Trivial()) |&gt; disp
6√ó2 Array{Float64,2}:
  1.157   0.379
  0.078   1.016
  0.216   1.664
 -0.163   1.566
 -1.197   0.274
 -0.468  -0.096
julia&gt; reshape(t1[], dim(codomain(t1)), dim(domain(t1))) |&gt; disp
6√ó2 Array{Float64,2}:
  1.157   0.379
  0.078   1.016
  0.216   1.664
 -0.163   1.566
 -1.197   0.274
 -0.468  -0.096</code></pre><p>Finally, all constructors can also be replaced by <code>Tensor(..., codomain)</code>, in which case the domain is assumed to be the empty <code>ProductSpace{S,0}()</code>, which can easily be obtained as <code>one(codomain)</code>. Indeed, the empty product space is the unit object of the monoidal category, equivalent to the field of scalars <code>ùïú</code>, and thus the multiplicative identity (especially since <code>*</code> also acts as tensor product on vector spaces).</p><p>The matrices created by <code>f</code> are the matrices <span>$B_c$</span> discussed above, i.e. those returned by <code>block(t, c)</code>. Only numerical matrices of type <code>DenseMatrix</code> are accepted, which in practice just means Julia&#39;s intrinsic <code>Matrix{T}</code> for some <code>T&lt;:Number</code>. In the future, we will add support for <code>CuMatrix</code> from <a href="https://github.com/JuliaGPU/CuArrays.jl">CuArrays.jl</a> to harness GPU computing power, and maybe <code>SharedArray</code> from the Julia&#39;s <code>SharedArrays</code> standard library.</p><p>Support for static or sparse data is currently not available, and if it would be implemented, it would lead to new subtypes of <code>AbstractTensorMap</code> which are distinct from <code>TensorMap</code>.</p><p>Let&#39;s conclude this section with some examples with <code>RepresentationSpace</code>.</p><pre><code class="language-julia-repl">julia&gt; V1 = ‚Ñ§‚ÇÇSpace(0=&gt;3,1=&gt;2)
‚Ñ§‚ÇÇSpace(0=&gt;3, 1=&gt;2)

julia&gt; V2 = ‚Ñ§‚ÇÇSpace(0=&gt;2,1=&gt;1)
‚Ñ§‚ÇÇSpace(0=&gt;2, 1=&gt;1)

julia&gt; t = TensorMap(randn, V1 ‚äó V1, V2 ‚äó V2&#39;)
TensorMap((‚Ñ§‚ÇÇSpace(0=&gt;3, 1=&gt;2) ‚äó ‚Ñ§‚ÇÇSpace(0=&gt;3, 1=&gt;2)) ‚Üê (‚Ñ§‚ÇÇSpace(0=&gt;2, 1=&gt;1) ‚äó ‚Ñ§‚ÇÇSpace(0=&gt;2, 1=&gt;1)&#39;)):
* Data for sector (‚Ñ§‚ÇÇ(1), ‚Ñ§‚ÇÇ(1)) ‚Üê (‚Ñ§‚ÇÇ(0), ‚Ñ§‚ÇÇ(0)):
[:, :, 1, 1] =
 -1.3814362850805104  2.563603033273739
 -0.540654752693751   0.4764466736542636

[:, :, 2, 1] =
 -1.2140802501385033  0.28040159911257084
 -0.7139254747863105  0.9874576796742743

[:, :, 1, 2] =
 -1.354674778676384    -0.8252256030789351
  0.45717202623467557   0.7097417804825575

[:, :, 2, 2] =
 1.8719014067858077  -0.21864980161259986
 1.281806169937053   -1.3302013498333027
* Data for sector (‚Ñ§‚ÇÇ(0), ‚Ñ§‚ÇÇ(0)) ‚Üê (‚Ñ§‚ÇÇ(0), ‚Ñ§‚ÇÇ(0)):
[:, :, 1, 1] =
  0.05735489616472927  -0.6585274051479906  -0.1734103631069051
 -0.645221935830563    -0.5878466534079121  -0.9732667530315307
 -0.08718374370767122  -1.159988478425593   -0.4075487015280168

[:, :, 2, 1] =
 -0.35052132754266047  -0.37477315769026515   0.878444469665126
 -0.13277520611814023  -0.6649470284865272   -0.05026306714285583
 -1.0650169068180577   -0.19885246698548303   2.1016918727969216

[:, :, 1, 2] =
 -0.10545881287568976  -1.1966500048741742   -0.3264157413179081
 -1.2300742101804254   -0.16340284532220384  -0.9726791203808213
 -0.7635967825163142    0.17372718343448598  -0.4834262004224127

[:, :, 2, 2] =
 -1.8229530447066709    -0.8468155256288133    1.767331683742684
 -1.1383066743949313     2.1826679489093648   -0.24782413397970027
 -0.006625815708274046   0.15241267815498288  -1.476949143691719
* Data for sector (‚Ñ§‚ÇÇ(1), ‚Ñ§‚ÇÇ(1)) ‚Üê (‚Ñ§‚ÇÇ(1), ‚Ñ§‚ÇÇ(1)):
[:, :, 1, 1] =
 -0.427670870942314     0.4021139729438053
 -0.41647661750933723  -0.5312627840050574
* Data for sector (‚Ñ§‚ÇÇ(0), ‚Ñ§‚ÇÇ(0)) ‚Üê (‚Ñ§‚ÇÇ(1), ‚Ñ§‚ÇÇ(1)):
[:, :, 1, 1] =
 -2.164366077183466    0.1957989924113869    1.282611291377071
 -0.3641650248261362  -0.11228357664978711   0.5890263948960155
 -1.1147504649493132  -0.33281491397979607  -1.5035040252404683
* Data for sector (‚Ñ§‚ÇÇ(1), ‚Ñ§‚ÇÇ(0)) ‚Üê (‚Ñ§‚ÇÇ(1), ‚Ñ§‚ÇÇ(0)):
[:, :, 1, 1] =
  0.27095857753155406   0.09672468637331341   0.048187457402160204
 -1.7827221448749586   -0.705339304373233    -0.22120794844021918

[:, :, 1, 2] =
  0.5619439467765459  0.04980895802518535   0.5421746338046768
 -0.3004397856274505  0.39775158102837843  -1.9779609776831704
* Data for sector (‚Ñ§‚ÇÇ(0), ‚Ñ§‚ÇÇ(1)) ‚Üê (‚Ñ§‚ÇÇ(1), ‚Ñ§‚ÇÇ(0)):
[:, :, 1, 1] =
  0.047513514718136515   0.6443957400270452
 -1.6135163270514907     1.7941374985162164
  0.0297028887026753    -0.48490119112282376

[:, :, 1, 2] =
 -1.842316726303085    1.6019616622194288
 -0.3858847493284501   0.45451858220053243
 -0.25644940545108086  0.1500263614162847
* Data for sector (‚Ñ§‚ÇÇ(1), ‚Ñ§‚ÇÇ(0)) ‚Üê (‚Ñ§‚ÇÇ(0), ‚Ñ§‚ÇÇ(1)):
[:, :, 1, 1] =
 -0.6887729039907621   0.7806199248596672   -1.0870452252318763
 -0.8950293008905944  -0.11692723091214893   0.3226003024786936

[:, :, 2, 1] =
 0.7335456609185034   -0.174498138238421   0.31069210737732295
 0.39542802155439716  -0.2521001358541179  1.6229249855224133
* Data for sector (‚Ñ§‚ÇÇ(0), ‚Ñ§‚ÇÇ(1)) ‚Üê (‚Ñ§‚ÇÇ(0), ‚Ñ§‚ÇÇ(1)):
[:, :, 1, 1] =
  1.4835506756307486  -0.4815104098161156
 -1.5424191359122492   0.3828850760928464
 -2.3020853215062482   1.537329611166185

[:, :, 2, 1] =
 -0.9853133685337933  0.5546225726526794
  0.3110849911802893  1.0309670446283674
  1.096194866392938   0.4050634872981833

julia&gt; (array = convert(Array, t)) |&gt; disp
5√ó5√ó3√ó3 Array{Float64,4}:
[:, :, 1, 1] =
  0.057  -0.658  -0.173   0.0    0.0  
 -0.645  -0.587  -0.973   0.0    0.0  
 -0.087  -1.159  -0.407   0.0    0.0  
  0.0     0.0     0.0    -1.381  2.563
  0.0     0.0     0.0    -0.54   0.476

[:, :, 2, 1] =
 -0.35   -0.374   0.878   0.0    0.0  
 -0.132  -0.664  -0.05    0.0    0.0  
 -1.065  -0.198   2.101   0.0    0.0  
  0.0     0.0     0.0    -1.214  0.28 
  0.0     0.0     0.0    -0.713  0.987

[:, :, 3, 1] =
  0.0     0.0     0.0     0.047   0.644
  0.0     0.0     0.0    -1.613   1.794
  0.0     0.0     0.0     0.029  -0.484
  0.27    0.096   0.048   0.0     0.0  
 -1.782  -0.705  -0.221   0.0     0.0  

[:, :, 1, 2] =
 -0.105  -1.196  -0.326   0.0     0.0  
 -1.23   -0.163  -0.972   0.0     0.0  
 -0.763   0.173  -0.483   0.0     0.0  
  0.0     0.0     0.0    -1.354  -0.825
  0.0     0.0     0.0     0.457   0.709

[:, :, 2, 2] =
 -1.822  -0.846   1.767  0.0     0.0  
 -1.138   2.182  -0.247  0.0     0.0  
 -0.006   0.152  -1.476  0.0     0.0  
  0.0     0.0     0.0    1.871  -0.218
  0.0     0.0     0.0    1.281  -1.33 

[:, :, 3, 2] =
  0.0    0.0     0.0    -1.842  1.601
  0.0    0.0     0.0    -0.385  0.454
  0.0    0.0     0.0    -0.256  0.15 
  0.561  0.049   0.542   0.0    0.0  
 -0.3    0.397  -1.977   0.0    0.0  

[:, :, 1, 3] =
  0.0     0.0     0.0     1.483  -0.481
  0.0     0.0     0.0    -1.542   0.382
  0.0     0.0     0.0    -2.302   1.537
 -0.688   0.78   -1.087   0.0     0.0  
 -0.895  -0.116   0.322   0.0     0.0  

[:, :, 2, 3] =
 0.0     0.0    0.0    -0.985  0.554
 0.0     0.0    0.0     0.311  1.03 
 0.0     0.0    0.0     1.096  0.405
 0.733  -0.174  0.31    0.0    0.0  
 0.395  -0.252  1.622   0.0    0.0  

[:, :, 3, 3] =
 -2.164   0.195   1.282   0.0     0.0  
 -0.364  -0.112   0.589   0.0     0.0  
 -1.114  -0.332  -1.503   0.0     0.0  
  0.0     0.0     0.0    -0.427   0.402
  0.0     0.0     0.0    -0.416  -0.531
julia&gt; d1 = dim(codomain(t))
25

julia&gt; d2 = dim(domain(t))
9

julia&gt; (matrix = reshape(array, d1, d2)) |&gt; disp
25√ó9 Array{Float64,2}:
  0.057  -0.35    0.0    -0.105  -1.822   0.0     0.0     0.0    -2.164
 -0.645  -0.132   0.0    -1.23   -1.138   0.0     0.0     0.0    -0.364
 -0.087  -1.065   0.0    -0.763  -0.006   0.0     0.0     0.0    -1.114
  0.0     0.0     0.27    0.0     0.0     0.561  -0.688   0.733   0.0  
  0.0     0.0    -1.782   0.0     0.0    -0.3    -0.895   0.395   0.0  
 -0.658  -0.374   0.0    -1.196  -0.846   0.0     0.0     0.0     0.195
 -0.587  -0.664   0.0    -0.163   2.182   0.0     0.0     0.0    -0.112
 -1.159  -0.198   0.0     0.173   0.152   0.0     0.0     0.0    -0.332
  0.0     0.0     0.096   0.0     0.0     0.049   0.78   -0.174   0.0  
  0.0     0.0    -0.705   0.0     0.0     0.397  -0.116  -0.252   0.0  
 -0.173   0.878   0.0    -0.326   1.767   0.0     0.0     0.0     1.282
 -0.973  -0.05    0.0    -0.972  -0.247   0.0     0.0     0.0     0.589
 -0.407   2.101   0.0    -0.483  -1.476   0.0     0.0     0.0    -1.503
  0.0     0.0     0.048   0.0     0.0     0.542  -1.087   0.31    0.0  
  0.0     0.0    -0.221   0.0     0.0    -1.977   0.322   1.622   0.0  
  0.0     0.0     0.047   0.0     0.0    -1.842   1.483  -0.985   0.0  
  0.0     0.0    -1.613   0.0     0.0    -0.385  -1.542   0.311   0.0  
  0.0     0.0     0.029   0.0     0.0    -0.256  -2.302   1.096   0.0  
 -1.381  -1.214   0.0    -1.354   1.871   0.0     0.0     0.0    -0.427
 -0.54   -0.713   0.0     0.457   1.281   0.0     0.0     0.0    -0.416
  0.0     0.0     0.644   0.0     0.0     1.601  -0.481   0.554   0.0  
  0.0     0.0     1.794   0.0     0.0     0.454   0.382   1.03    0.0  
  0.0     0.0    -0.484   0.0     0.0     0.15    1.537   0.405   0.0  
  2.563   0.28    0.0    -0.825  -0.218   0.0     0.0     0.0     0.402
  0.476   0.987   0.0     0.709  -1.33    0.0     0.0     0.0    -0.531
julia&gt; (u = reshape(convert(Array, TensorMap(I, codomain(t), fuse(codomain(t)))), d1, d1)) |&gt; disp
25√ó25 Array{Float64,2}:
 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
julia&gt; (v = reshape(convert(Array, TensorMap(I, domain(t), fuse(domain(t)))), d2, d2)) |&gt; disp
9√ó9 Array{Float64,2}:
 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0
 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0
 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0
julia&gt; u&#39;*u ‚âà I ‚âà v&#39;*v
true

julia&gt; (u&#39;*matrix*v) |&gt; disp
25√ó9 Array{Float64,2}:
  0.057  -0.35   -0.105  -1.822  -2.164   0.0     0.0     0.0     0.0  
 -0.645  -0.132  -1.23   -1.138  -0.364   0.0     0.0     0.0     0.0  
 -0.087  -1.065  -0.763  -0.006  -1.114   0.0     0.0     0.0     0.0  
 -0.658  -0.374  -1.196  -0.846   0.195   0.0     0.0     0.0     0.0  
 -0.587  -0.664  -0.163   2.182  -0.112   0.0     0.0     0.0     0.0  
 -1.159  -0.198   0.173   0.152  -0.332   0.0     0.0     0.0     0.0  
 -0.173   0.878  -0.326   1.767   1.282   0.0     0.0     0.0     0.0  
 -0.973  -0.05   -0.972  -0.247   0.589   0.0     0.0     0.0     0.0  
 -0.407   2.101  -0.483  -1.476  -1.503   0.0     0.0     0.0     0.0  
 -1.381  -1.214  -1.354   1.871  -0.427   0.0     0.0     0.0     0.0  
 -0.54   -0.713   0.457   1.281  -0.416   0.0     0.0     0.0     0.0  
  2.563   0.28   -0.825  -0.218   0.402   0.0     0.0     0.0     0.0  
  0.476   0.987   0.709  -1.33   -0.531   0.0     0.0     0.0     0.0  
  0.0     0.0     0.0     0.0     0.0     0.27    0.561  -0.688   0.733
  0.0     0.0     0.0     0.0     0.0    -1.782  -0.3    -0.895   0.395
  0.0     0.0     0.0     0.0     0.0     0.096   0.049   0.78   -0.174
  0.0     0.0     0.0     0.0     0.0    -0.705   0.397  -0.116  -0.252
  0.0     0.0     0.0     0.0     0.0     0.048   0.542  -1.087   0.31 
  0.0     0.0     0.0     0.0     0.0    -0.221  -1.977   0.322   1.622
  0.0     0.0     0.0     0.0     0.0     0.047  -1.842   1.483  -0.985
  0.0     0.0     0.0     0.0     0.0    -1.613  -0.385  -1.542   0.311
  0.0     0.0     0.0     0.0     0.0     0.029  -0.256  -2.302   1.096
  0.0     0.0     0.0     0.0     0.0     0.644   1.601  -0.481   0.554
  0.0     0.0     0.0     0.0     0.0     1.794   0.454   0.382   1.03 
  0.0     0.0     0.0     0.0     0.0    -0.484   0.15    1.537   0.405
julia&gt; # compare with:
       block(t, ‚Ñ§‚ÇÇ(0)) |&gt; disp
13√ó5 Array{Float64,2}:
  0.057  -0.35   -0.105  -1.822  -2.164
 -0.645  -0.132  -1.23   -1.138  -0.364
 -0.087  -1.065  -0.763  -0.006  -1.114
 -0.658  -0.374  -1.196  -0.846   0.195
 -0.587  -0.664  -0.163   2.182  -0.112
 -1.159  -0.198   0.173   0.152  -0.332
 -0.173   0.878  -0.326   1.767   1.282
 -0.973  -0.05   -0.972  -0.247   0.589
 -0.407   2.101  -0.483  -1.476  -1.503
 -1.381  -1.214  -1.354   1.871  -0.427
 -0.54   -0.713   0.457   1.281  -0.416
  2.563   0.28   -0.825  -0.218   0.402
  0.476   0.987   0.709  -1.33   -0.531
julia&gt; block(t, ‚Ñ§‚ÇÇ(1)) |&gt; disp
12√ó4 Array{Float64,2}:
  0.27    0.561  -0.688   0.733
 -1.782  -0.3    -0.895   0.395
  0.096   0.049   0.78   -0.174
 -0.705   0.397  -0.116  -0.252
  0.048   0.542  -1.087   0.31 
 -0.221  -1.977   0.322   1.622
  0.047  -1.842   1.483  -0.985
 -1.613  -0.385  -1.542   0.311
  0.029  -0.256  -2.302   1.096
  0.644   1.601  -0.481   0.554
  1.794   0.454   0.382   1.03 
 -0.484   0.15    1.537   0.405</code></pre><p>Here, we illustrated some additional concepts. We constructed a <code>TensorMap</code> where the blocks are initialized with the identity matrix using <code>I::UniformScaling</code> from Julia&#39;s <code>LinearAlgebra</code> standard library. This works even if the blocks are not square, in this case zero rows or columns (depending on the shape of the block) will be added. Creating a <code>TensorMap</code> with <code>I</code> is a useful way to construct a fixed unitary or isometry between two spaces. The operation <code>fuse(V)</code> creates an <code>ElementarySpace</code> which is isomorphic to a given space <code>V</code> (of type <code>ProductSpace</code> or <code>ElementarySpace</code>). Constructing a <code>TensorMap</code> between <code>V</code> and <code>fuse(V)</code> using the <code>I</code> constructor definitely results in a unitary, in particular it is the unitary which implements the basis change from the product basis to the coupled basis. In this case, for a group <code>G</code> with <code>FusionStyle(G) isa Abelian</code>, it is a permutation matrix. Specifically choosing <code>V</code> equal to the codomain and domain of <code>t</code>, we can construct the explicit basis transforms that bring <code>t</code> into block diagonal form.</p><p>Let&#39;s repeat the same exercise for <code>G = SU‚ÇÇ</code>, which has <code>FusionStyle(G) isa NonAbelian</code>.</p><pre><code class="language-julia-repl">julia&gt; V1 = SU‚ÇÇSpace(0=&gt;2,1=&gt;1)
SU‚ÇÇSpace(0=&gt;2, 1=&gt;1)

julia&gt; V2 = SU‚ÇÇSpace(0=&gt;1,1=&gt;1)
SU‚ÇÇSpace(0=&gt;1, 1=&gt;1)

julia&gt; t = TensorMap(randn, V1 ‚äó V1, V2 ‚äó V2&#39;)
TensorMap((SU‚ÇÇSpace(0=&gt;2, 1=&gt;1) ‚äó SU‚ÇÇSpace(0=&gt;2, 1=&gt;1)) ‚Üê (SU‚ÇÇSpace(0=&gt;1, 1=&gt;1) ‚äó SU‚ÇÇSpace(0=&gt;1, 1=&gt;1)&#39;)):
* Data for fusiontree FusionTree{SU‚ÇÇ}((1, 1), 0, (false, false), ()) ‚Üê FusionTree{SU‚ÇÇ}((0, 0), 0, (false, true), ()):
[:, :, 1, 1] =
 1.779704834270866
* Data for fusiontree FusionTree{SU‚ÇÇ}((0, 0), 0, (false, false), ()) ‚Üê FusionTree{SU‚ÇÇ}((0, 0), 0, (false, true), ()):
[:, :, 1, 1] =
 -0.15510731296868036  1.019328459611239
  0.13269573263103934  0.6325406632904298
* Data for fusiontree FusionTree{SU‚ÇÇ}((1, 1), 0, (false, false), ()) ‚Üê FusionTree{SU‚ÇÇ}((1, 1), 0, (false, true), ()):
[:, :, 1, 1] =
 2.2292148285061315
* Data for fusiontree FusionTree{SU‚ÇÇ}((0, 0), 0, (false, false), ()) ‚Üê FusionTree{SU‚ÇÇ}((1, 1), 0, (false, true), ()):
[:, :, 1, 1] =
 -0.45201925175437935  -2.8400664451152435
 -1.868320950162022    -1.9508025132296634
* Data for fusiontree FusionTree{SU‚ÇÇ}((1, 0), 1, (false, false), ()) ‚Üê FusionTree{SU‚ÇÇ}((1, 0), 1, (false, true), ()):
[:, :, 1, 1] =
 -1.1948062772198769  -0.9784879649204328
* Data for fusiontree FusionTree{SU‚ÇÇ}((0, 1), 1, (false, false), ()) ‚Üê FusionTree{SU‚ÇÇ}((1, 0), 1, (false, true), ()):
[:, :, 1, 1] =
 0.46836385573614536
 1.1245173028374533
* Data for fusiontree FusionTree{SU‚ÇÇ}((1, 1), 1, (false, false), ()) ‚Üê FusionTree{SU‚ÇÇ}((1, 0), 1, (false, true), ()):
[:, :, 1, 1] =
 0.28477757109007046
* Data for fusiontree FusionTree{SU‚ÇÇ}((1, 0), 1, (false, false), ()) ‚Üê FusionTree{SU‚ÇÇ}((0, 1), 1, (false, true), ()):
[:, :, 1, 1] =
 -1.4492735116601974  1.5039942313268193
* Data for fusiontree FusionTree{SU‚ÇÇ}((0, 1), 1, (false, false), ()) ‚Üê FusionTree{SU‚ÇÇ}((0, 1), 1, (false, true), ()):
[:, :, 1, 1] =
 -0.2105129208566306
 -1.9541332451980071
* Data for fusiontree FusionTree{SU‚ÇÇ}((1, 1), 1, (false, false), ()) ‚Üê FusionTree{SU‚ÇÇ}((0, 1), 1, (false, true), ()):
[:, :, 1, 1] =
 0.8336758983471871
* Data for fusiontree FusionTree{SU‚ÇÇ}((1, 0), 1, (false, false), ()) ‚Üê FusionTree{SU‚ÇÇ}((1, 1), 1, (false, true), ()):
[:, :, 1, 1] =
 0.0336327884658348  -0.2600062400896797
* Data for fusiontree FusionTree{SU‚ÇÇ}((0, 1), 1, (false, false), ()) ‚Üê FusionTree{SU‚ÇÇ}((1, 1), 1, (false, true), ()):
[:, :, 1, 1] =
 -0.6839004389021909
 -0.6289671576669886
* Data for fusiontree FusionTree{SU‚ÇÇ}((1, 1), 1, (false, false), ()) ‚Üê FusionTree{SU‚ÇÇ}((1, 1), 1, (false, true), ()):
[:, :, 1, 1] =
 -0.6199106663248221
* Data for fusiontree FusionTree{SU‚ÇÇ}((1, 1), 2, (false, false), ()) ‚Üê FusionTree{SU‚ÇÇ}((1, 1), 2, (false, true), ()):
[:, :, 1, 1] =
 -0.4515970777478777

julia&gt; (array = convert(Array, t)) |&gt; disp
5√ó5√ó4√ó4 Array{Float64,4}:
[:, :, 1, 1] =
 -0.155  1.019  0.0     0.0    0.0  
  0.132  0.632  0.0     0.0    0.0  
  0.0    0.0    0.0     0.0    1.027
  0.0    0.0    0.0    -1.027  0.0  
  0.0    0.0    1.027   0.0    0.0  

[:, :, 2, 1] =
  0.0     0.0     0.468  0.0    0.0
  0.0     0.0     1.124  0.0    0.0
 -1.194  -0.978   0.0    0.201  0.0
  0.0     0.0    -0.201  0.0    0.0
  0.0     0.0     0.0    0.0    0.0

[:, :, 3, 1] =
  0.0     0.0     0.0    0.468  0.0  
  0.0     0.0     0.0    1.124  0.0  
  0.0     0.0     0.0    0.0    0.201
 -1.194  -0.978   0.0    0.0    0.0  
  0.0     0.0    -0.201  0.0    0.0  

[:, :, 4, 1] =
  0.0     0.0    0.0   0.0    0.468
  0.0     0.0    0.0   0.0    1.124
  0.0     0.0    0.0   0.0    0.0  
  0.0     0.0    0.0   0.0    0.201
 -1.194  -0.978  0.0  -0.201  0.0  

[:, :, 1, 2] =
  0.0    0.0    0.0   0.0    -0.21 
  0.0    0.0    0.0   0.0    -1.954
  0.0    0.0    0.0   0.0     0.0  
  0.0    0.0    0.0   0.0     0.589
 -1.449  1.503  0.0  -0.589   0.0  

[:, :, 2, 2] =
 -0.26   -1.639  0.0    -0.483  0.0  
 -1.078  -1.126  0.0    -0.444  0.0  
  0.0     0.0    0.0     0.0    0.357
  0.023  -0.183  0.0    -0.893  0.0  
  0.0     0.0    0.977   0.0    0.0  

[:, :, 3, 2] =
 0.0     0.0    0.0  0.0    -0.483
 0.0     0.0    0.0  0.0    -0.444
 0.0     0.0    0.0  0.0     0.0  
 0.0     0.0    0.0  0.0    -0.535
 0.023  -0.183  0.0  0.084   0.0  

[:, :, 4, 2] =
 0.0  0.0  0.0  0.0   0.0  
 0.0  0.0  0.0  0.0   0.0  
 0.0  0.0  0.0  0.0   0.0  
 0.0  0.0  0.0  0.0   0.0  
 0.0  0.0  0.0  0.0  -0.451

[:, :, 1, 3] =
 0.0     0.0    0.0    0.21    0.0  
 0.0     0.0    0.0    1.954   0.0  
 0.0     0.0    0.0    0.0    -0.589
 1.449  -1.503  0.0    0.0     0.0  
 0.0     0.0    0.589  0.0     0.0  

[:, :, 2, 3] =
  0.0    0.0     0.483  0.0    0.0
  0.0    0.0     0.444  0.0    0.0
 -0.023  0.183   0.0    0.535  0.0
  0.0    0.0    -0.084  0.0    0.0
  0.0    0.0     0.0    0.0    0.0

[:, :, 3, 3] =
 -0.26   -1.639  0.0     0.0    0.0  
 -1.078  -1.126  0.0     0.0    0.0  
  0.0     0.0    0.0     0.0    0.893
  0.0     0.0    0.0    -0.442  0.0  
  0.0     0.0    0.893   0.0    0.0  

[:, :, 4, 3] =
 0.0     0.0    0.0  0.0    -0.483
 0.0     0.0    0.0  0.0    -0.444
 0.0     0.0    0.0  0.0     0.0  
 0.0     0.0    0.0  0.0    -0.084
 0.023  -0.183  0.0  0.535   0.0  

[:, :, 1, 4] =
  0.0    0.0    -0.21   0.0    0.0
  0.0    0.0    -1.954  0.0    0.0
 -1.449  1.503   0.0    0.589  0.0
  0.0    0.0    -0.589  0.0    0.0
  0.0    0.0     0.0    0.0    0.0

[:, :, 2, 4] =
 0.0  0.0   0.0    0.0  0.0
 0.0  0.0   0.0    0.0  0.0
 0.0  0.0  -0.451  0.0  0.0
 0.0  0.0   0.0    0.0  0.0
 0.0  0.0   0.0    0.0  0.0

[:, :, 3, 4] =
  0.0    0.0     0.483  0.0    0.0
  0.0    0.0     0.444  0.0    0.0
 -0.023  0.183   0.0    0.084  0.0
  0.0    0.0    -0.535  0.0    0.0
  0.0    0.0     0.0    0.0    0.0

[:, :, 4, 4] =
 -0.26   -1.639  0.0     0.483  0.0  
 -1.078  -1.126  0.0     0.444  0.0  
  0.0     0.0    0.0     0.0    0.977
 -0.023   0.183  0.0    -0.893  0.0  
  0.0     0.0    0.357   0.0    0.0  
julia&gt; d1 = dim(codomain(t))
25

julia&gt; d2 = dim(domain(t))
16

julia&gt; (matrix = reshape(array, d1, d2)) |&gt; disp
25√ó16 Array{Float64,2}:
 -0.155   0.0     0.0     0.0     0.0    -0.26    0.0     0.0     0.0     0.0    -0.26    0.0     0.0     0.0     0.0    -0.26 
  0.132   0.0     0.0     0.0     0.0    -1.078   0.0     0.0     0.0     0.0    -1.078   0.0     0.0     0.0     0.0    -1.078
  0.0    -1.194   0.0     0.0     0.0     0.0     0.0     0.0     0.0    -0.023   0.0     0.0    -1.449   0.0    -0.023   0.0  
  0.0     0.0    -1.194   0.0     0.0     0.023   0.0     0.0     1.449   0.0     0.0     0.0     0.0     0.0     0.0    -0.023
  0.0     0.0     0.0    -1.194  -1.449   0.0     0.023   0.0     0.0     0.0     0.0     0.023   0.0     0.0     0.0     0.0  
  1.019   0.0     0.0     0.0     0.0    -1.639   0.0     0.0     0.0     0.0    -1.639   0.0     0.0     0.0     0.0    -1.639
  0.632   0.0     0.0     0.0     0.0    -1.126   0.0     0.0     0.0     0.0    -1.126   0.0     0.0     0.0     0.0    -1.126
  0.0    -0.978   0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.183   0.0     0.0     1.503   0.0     0.183   0.0  
  0.0     0.0    -0.978   0.0     0.0    -0.183   0.0     0.0    -1.503   0.0     0.0     0.0     0.0     0.0     0.0     0.183
  0.0     0.0     0.0    -0.978   1.503   0.0    -0.183   0.0     0.0     0.0     0.0    -0.183   0.0     0.0     0.0     0.0  
  0.0     0.468   0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.483   0.0     0.0    -0.21    0.0     0.483   0.0  
  0.0     1.124   0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.444   0.0     0.0    -1.954   0.0     0.444   0.0  
  0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0    -0.451   0.0     0.0  
  0.0    -0.201   0.0     0.0     0.0     0.0     0.0     0.0     0.0    -0.084   0.0     0.0    -0.589   0.0    -0.535   0.0  
  1.027   0.0    -0.201   0.0     0.0     0.977   0.0     0.0     0.589   0.0     0.893   0.0     0.0     0.0     0.0     0.357
  0.0     0.0     0.468   0.0     0.0    -0.483   0.0     0.0     0.21    0.0     0.0     0.0     0.0     0.0     0.0     0.483
  0.0     0.0     1.124   0.0     0.0    -0.444   0.0     0.0     1.954   0.0     0.0     0.0     0.0     0.0     0.0     0.444
  0.0     0.201   0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.535   0.0     0.0     0.589   0.0     0.084   0.0  
 -1.027   0.0     0.0     0.0     0.0    -0.893   0.0     0.0     0.0     0.0    -0.442   0.0     0.0     0.0     0.0    -0.893
  0.0     0.0     0.0    -0.201  -0.589   0.0     0.084   0.0     0.0     0.0     0.0     0.535   0.0     0.0     0.0     0.0  
  0.0     0.0     0.0     0.468  -0.21    0.0    -0.483   0.0     0.0     0.0     0.0    -0.483   0.0     0.0     0.0     0.0  
  0.0     0.0     0.0     1.124  -1.954   0.0    -0.444   0.0     0.0     0.0     0.0    -0.444   0.0     0.0     0.0     0.0  
  1.027   0.0     0.201   0.0     0.0     0.357   0.0     0.0    -0.589   0.0     0.893   0.0     0.0     0.0     0.0     0.977
  0.0     0.0     0.0     0.201   0.589   0.0    -0.535   0.0     0.0     0.0     0.0    -0.084   0.0     0.0     0.0     0.0  
  0.0     0.0     0.0     0.0     0.0     0.0     0.0    -0.451   0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  
julia&gt; (u = reshape(convert(Array, TensorMap(I, codomain(t), fuse(codomain(t)))), d1, d1)) |&gt; disp
25√ó25 Array{Float64,2}:
 1.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  1.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.0    1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  1.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  1.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    1.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  -0.707   0.0     0.0    0.0  0.707  0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.577  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    -0.707   0.0    0.0  0.0    0.408  0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.707   0.0     0.0    0.0  0.707  0.0    0.0    0.0
 0.0  0.0  0.0  0.0  -0.577  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.816  0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0    -0.707  0.0  0.0    0.0    0.707  0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    0.0
 0.0  0.0  0.0  0.0   0.577  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.707   0.0    0.0  0.0    0.408  0.0    0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.707  0.0  0.0    0.0    0.707  0.0
 0.0  0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0     0.0    0.0  0.0    0.0    0.0    1.0
julia&gt; (v = reshape(convert(Array, TensorMap(I, domain(t), fuse(domain(t)))), d2, d2)) |&gt; disp
16√ó16 Array{Float64,2}:
 1.0  0.0    0.0  0.0  0.0  0.0     0.0    0.0     0.0     0.0    0.0    0.0     0.0     0.0     0.0    0.0  
 0.0  0.0    1.0  0.0  0.0  0.0     0.0    0.0     0.0     0.0    0.0    0.0     0.0     0.0     0.0    0.0  
 0.0  0.0    0.0  1.0  0.0  0.0     0.0    0.0     0.0     0.0    0.0    0.0     0.0     0.0     0.0    0.0  
 0.0  0.0    0.0  0.0  1.0  0.0     0.0    0.0     0.0     0.0    0.0    0.0     0.0     0.0     0.0    0.0  
 0.0  0.0    0.0  0.0  0.0  0.0     0.0    0.999   0.0     0.0    0.0    0.0     0.0     0.0     0.0    0.0  
 0.0  0.577  0.0  0.0  0.0  0.0     0.0    0.0     0.0     0.707  0.0    0.0     0.0     0.408   0.0    0.0  
 0.0  0.0    0.0  0.0  0.0  0.0     0.0    0.0     0.0     0.0    0.707  0.0     0.0     0.0     0.707  0.0  
 0.0  0.0    0.0  0.0  0.0  0.0     0.0    0.0     0.0     0.0    0.0    0.0     0.0     0.0     0.0    0.999
 0.0  0.0    0.0  0.0  0.0  0.0    -0.999  0.0     0.0     0.0    0.0    0.0     0.0     0.0     0.0    0.0  
 0.0  0.0    0.0  0.0  0.0  0.0     0.0    0.0    -0.707   0.0    0.0    0.0    -0.707   0.0     0.0    0.0  
 0.0  0.577  0.0  0.0  0.0  0.0     0.0    0.0     0.0     0.0    0.0    0.0     0.0    -0.816   0.0    0.0  
 0.0  0.0    0.0  0.0  0.0  0.0     0.0    0.0     0.0     0.0    0.707  0.0     0.0     0.0    -0.707  0.0  
 0.0  0.0    0.0  0.0  0.0  0.999   0.0    0.0     0.0     0.0    0.0    0.0     0.0     0.0     0.0    0.0  
 0.0  0.0    0.0  0.0  0.0  0.0     0.0    0.0     0.0     0.0    0.0    0.999   0.0     0.0     0.0    0.0  
 0.0  0.0    0.0  0.0  0.0  0.0     0.0    0.0    -0.707   0.0    0.0    0.0     0.707   0.0     0.0    0.0  
 0.0  0.577  0.0  0.0  0.0  0.0     0.0    0.0     0.0    -0.707  0.0    0.0     0.0     0.408   0.0    0.0  
julia&gt; u&#39;*u ‚âà I ‚âà v&#39;*v
true

julia&gt; (u&#39;*matrix*v) |&gt; disp
25√ó16 Array{Float64,2}:
 -0.155  -0.452   0.0     0.0     0.0     0.0     0.0     0.0     0.0    -0.0     0.0     0.0     0.0    -0.0     0.0     0.0  
  0.132  -1.868   0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0    -0.0     0.0     0.0  
  1.019  -2.84    0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0    -0.0     0.0     0.0  
  0.632  -1.95    0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0    -0.0     0.0     0.0  
  1.779   2.229   0.0     0.0     0.0     0.0     0.0     0.0     0.0    -0.0     0.0     0.0     0.0    -0.0     0.0     0.0  
  0.0     0.0    -1.194   0.0     0.0    -1.449   0.0     0.0     0.033   0.0     0.0     0.0    -0.0     0.0     0.0     0.0  
  0.0     0.0     0.0    -1.194   0.0     0.0    -1.449   0.0     0.0     0.033   0.0     0.0     0.0     0.0     0.0     0.0  
  0.0     0.0     0.0     0.0    -1.194   0.0     0.0    -1.449   0.0     0.0     0.033   0.0     0.0     0.0    -0.0     0.0  
  0.0     0.0    -0.978   0.0     0.0     1.503   0.0     0.0    -0.26    0.0     0.0     0.0     0.0     0.0     0.0     0.0  
  0.0    -0.0     0.0    -0.978   0.0     0.0     1.503   0.0     0.0    -0.26    0.0     0.0     0.0    -0.0     0.0     0.0  
  0.0     0.0     0.0     0.0    -0.978   0.0     0.0     1.503   0.0     0.0    -0.26    0.0     0.0     0.0     0.0     0.0  
  0.0     0.0     0.468   0.0     0.0    -0.21    0.0     0.0    -0.683   0.0     0.0     0.0     0.0     0.0     0.0     0.0  
  0.0    -0.0     0.0     0.468   0.0     0.0    -0.21    0.0     0.0    -0.683   0.0     0.0     0.0     0.0     0.0     0.0  
  0.0     0.0     0.0     0.0     0.468   0.0     0.0    -0.21    0.0     0.0    -0.683   0.0     0.0     0.0     0.0     0.0  
  0.0     0.0     1.124   0.0     0.0    -1.954   0.0     0.0    -0.628   0.0     0.0     0.0     0.0     0.0     0.0     0.0  
  0.0    -0.0     0.0     1.124   0.0     0.0    -1.954   0.0     0.0    -0.628   0.0     0.0     0.0     0.0     0.0     0.0  
  0.0     0.0     0.0     0.0     1.124   0.0     0.0    -1.954   0.0     0.0    -0.628   0.0     0.0     0.0     0.0     0.0  
  0.0     0.0     0.284   0.0     0.0     0.833   0.0     0.0    -0.619   0.0     0.0     0.0     0.0     0.0     0.0     0.0  
 -0.0     0.0     0.0     0.284   0.0     0.0     0.833   0.0     0.0    -0.619   0.0     0.0     0.0     0.0     0.0     0.0  
  0.0     0.0     0.0     0.0     0.284   0.0     0.0     0.833   0.0     0.0    -0.619   0.0     0.0     0.0     0.0     0.0  
  0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0    -0.451   0.0     0.0     0.0     0.0  
  0.0     0.0    -0.0     0.0     0.0     0.0     0.0     0.0    -0.0     0.0     0.0     0.0    -0.451   0.0     0.0     0.0  
  0.0    -0.0     0.0    -0.0     0.0     0.0    -0.0     0.0     0.0    -0.0     0.0     0.0     0.0    -0.451   0.0     0.0  
  0.0     0.0     0.0     0.0    -0.0     0.0     0.0     0.0     0.0     0.0    -0.0     0.0     0.0     0.0    -0.451   0.0  
  0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0    -0.451
julia&gt; # compare with:
       block(t, SU‚ÇÇ(0)) |&gt; disp
5√ó2 Array{Float64,2}:
 -0.155  -0.452
  0.132  -1.868
  1.019  -2.84 
  0.632  -1.95 
  1.779   2.229
julia&gt; block(t, SU‚ÇÇ(1)) |&gt; disp
5√ó3 Array{Float64,2}:
 -1.194  -1.449   0.033
 -0.978   1.503  -0.26 
  0.468  -0.21   -0.683
  1.124  -1.954  -0.628
  0.284   0.833  -0.619
julia&gt; block(t, SU‚ÇÇ(2)) |&gt; disp
1√ó1 Array{Float64,2}:
 -0.451</code></pre><p>Note that the basis transforms <code>u</code> and <code>v</code> are no longer permutation matrices, but are still unitary. Furthermore, note that they render the tensor block diagonal, but that now every element of the diagonal blocks labeled by <code>c</code> comes itself in a tensor product with an identity matrix of size <code>dim(c)</code>, i.e. <code>dim(SU‚ÇÇ(1)) = 3</code> and <code>dim(SU‚ÇÇ(2)) = 5</code>.</p><p>To create a <code>TensorMap</code> with existing data, one can use the aforementioned form but with the function <code>f</code> replaced with the actual data, i.e. <code>TensorMap(data, codomain, domain)</code> or any of its equivalents. For the specific form of <code>data</code>, we distinguish between the case without and with symmetry. In the former case, one can just pass a <code>DenseArray</code>, either of rank <code>N‚ÇÅ+N‚ÇÇ</code> and with matching size <code>(dims(codomain)..., dims(domain)...)</code>, or just as a <code>DenseMatrix</code> with size <code>(dim(codomain), dim(domain))</code>. In the case of symmetry, <code>data</code> needs to be specified as a dictionary (some subtype of <code>AbstractDict</code>) with the blocksectors <code>c::G &lt;: Sector</code> as keys and the corresponding matrix blocks as value, i.e. <code>data[c]</code> is some <code>DenseMatrix</code> of size <code>(blockdim(codomain, c), blockdim(domain, c))</code>.</p><pre><code class="language-julia-repl">julia&gt; data = randn(3,3,3)
3√ó3√ó3 Array{Float64,3}:
[:, :, 1] =
  0.518257  -0.0452636   0.520951
 -0.708514   1.71688     0.83841
  0.196601  -1.16986    -0.773959

[:, :, 2] =
  1.21584    0.729332  -0.400669
  0.682861  -0.222014   1.56714
 -0.459484  -1.66321   -0.640709

[:, :, 3] =
 -0.503201   0.767818   1.53971
  1.01274   -0.842443   0.912489
  1.15831   -0.622851  -0.0629279

julia&gt; t = TensorMap(data, ‚ÑÇ^3 ‚äó ‚ÑÇ^3, ‚ÑÇ^3)
TensorMap((‚ÑÇ^3 ‚äó ‚ÑÇ^3) ‚Üê ProductSpace(‚ÑÇ^3)):
[:, :, 1] =
  0.5182572514711414   -0.04526363553035746   0.5209506877467628
 -0.7085140603668081    1.7168755012424717    0.8384103022067444
  0.19660128449800568  -1.1698625331631045   -0.7739590970386925

[:, :, 2] =
  1.2158384257392438    0.7293323456759      -0.40066914750218846
  0.6828609291440884   -0.22201383096477792   1.5671353973615552
 -0.45948399749585683  -1.6632146994099128   -0.6407085940445624

[:, :, 3] =
 -0.5032014909835856   0.767818362085198    1.539712707923981
  1.012737918712819   -0.8424434731885723   0.9124888889884838
  1.1583141908895025  -0.622850908829003   -0.06292788133250982

julia&gt; t ‚âà TensorMap(reshape(data, (9, 3)), ‚ÑÇ^3 ‚äó ‚ÑÇ^3, ‚ÑÇ^3)
true

julia&gt; V = ‚Ñ§‚ÇÇSpace(0=&gt;2, 1=&gt;2)
‚Ñ§‚ÇÇSpace(0=&gt;2, 1=&gt;2)

julia&gt; data = Dict(‚Ñ§‚ÇÇ(0)=&gt;randn(8,2), ‚Ñ§‚ÇÇ(1)=&gt;randn(8,2))
Dict{‚Ñ§‚ÇÇ,Array{Float64,2}} with 2 entries:
  ‚Ñ§‚ÇÇ(0) =&gt; [0.072423 -0.068709; 0.953225 0.243609; ‚Ä¶ ; 0.816852 -0.0258699; -0.‚Ä¶
  ‚Ñ§‚ÇÇ(1) =&gt; [-2.15564 0.343366; 0.389641 2.35504; ‚Ä¶ ; 0.766565 0.21035; 1.34244 ‚Ä¶

julia&gt; t2 = TensorMap(data, V*V, V)
TensorMap((‚Ñ§‚ÇÇSpace(0=&gt;2, 1=&gt;2) ‚äó ‚Ñ§‚ÇÇSpace(0=&gt;2, 1=&gt;2)) ‚Üê ProductSpace(‚Ñ§‚ÇÇSpace(0=&gt;2, 1=&gt;2))):
* Data for sector (‚Ñ§‚ÇÇ(1), ‚Ñ§‚ÇÇ(1)) ‚Üê (‚Ñ§‚ÇÇ(0),):
[:, :, 1] =
 -0.715381542455454    0.8168520848578315
  0.6956693723258189  -0.887053335047302

[:, :, 2] =
 2.2241828261474343  -0.025869882407200097
 0.581366174238408   -0.35127630426002654
* Data for sector (‚Ñ§‚ÇÇ(0), ‚Ñ§‚ÇÇ(0)) ‚Üê (‚Ñ§‚ÇÇ(0),):
[:, :, 1] =
 0.07242297951883063  0.2724758160483822
 0.95322515870127     0.673125652792039

[:, :, 2] =
 -0.06870897771119824  -0.09701079770593127
  0.2436092958114225    1.4583504774231588
* Data for sector (‚Ñ§‚ÇÇ(1), ‚Ñ§‚ÇÇ(0)) ‚Üê (‚Ñ§‚ÇÇ(1),):
[:, :, 1] =
 -2.1556355882017675  -0.8519949397923275
  0.3896405395556132   1.2709801805442231

[:, :, 2] =
 0.3433662746487645  -1.2867502093733085
 2.3550376759087492   1.9183159286872375
* Data for sector (‚Ñ§‚ÇÇ(0), ‚Ñ§‚ÇÇ(1)) ‚Üê (‚Ñ§‚ÇÇ(1),):
[:, :, 1] =
 -1.1568064022097884   0.7665648468596464
 -0.34520981684377805  1.3424422926895838

[:, :, 2] =
 -0.5707299733259199   0.210350151464598
 -1.3077340945687843  -0.3139550485864195

julia&gt; for (c,b) in blocks(t2)
           println(&quot;Data for block $c :&quot;)
           b |&gt; disp
           println()
       end
Data for block ‚Ñ§‚ÇÇ(0) :
8√ó2 Array{Float64,2}:
  0.072  -0.068
  0.953   0.243
  0.272  -0.097
  0.673   1.458
 -0.715   2.224
  0.695   0.581
  0.816  -0.025
 -0.887  -0.351
Data for block ‚Ñ§‚ÇÇ(1) :
8√ó2 Array{Float64,2}:
 -2.155   0.343
  0.389   2.355
 -0.851  -1.286
  1.27    1.918
 -1.156  -0.57 
 -0.345  -1.307
  0.766   0.21 
  1.342  -0.313</code></pre><p>A third way to construct a <code>TensorMap</code> instance is to use <code>Base.similar</code>, i.e.</p><p><code>similar(t [, T::Type{&lt;:Number}, codomain, domain])</code></p><p>where <code>T</code> is a possibly different <code>eltype</code> for the tensor data, and <code>codomain</code> and <code>domain</code> optionally define a new codomain and domain for the resulting tensor. By default, these values just take the value from the input tensor <code>t</code>. The result will be a new <code>TensorMap</code> instance, with <code>undef</code> data, but whose data is stored in the same subtype of <code>DenseMatrix</code> (e.g. <code>Matrix</code> or <code>CuMatrix</code> or ...) as <code>t</code>. In particular, this uses the internal methods <code>TensorKit.storagetype(t)</code> and <code>TensorKit.similarstoragetype(t, T)</code>.</p><p>Finally, there are methods <code>zero</code>, <code>one</code>, <code>id</code>, <code>isomorphism</code> and <code>unitary</code> to create specific new tensors, and which are introduced in the section on <a href="#ss_tensor_linalg-1">linear algebra operations</a>.</p><h2 id="ss_tensor_properties-1"><a class="docs-heading-anchor" href="#ss_tensor_properties-1">Tensor properties</a><a class="docs-heading-anchor-permalink" href="#ss_tensor_properties-1" title="Permalink"></a></h2><p>Given a <code>t::AbstractTensorMap{S,N‚ÇÅ,N‚ÇÇ}</code>, there are various methods to query its properties. The most important are clearly <code>codomain(t)</code> and <code>domain(t)</code>. For <code>t::AbstractTensor{S,N}</code>, i.e. <code>t::AbstractTensorMap{S,N,0}</code>, we can use <code>space(t)</code> as synonym for <code>codomain(t)</code>. However, for a general <code>AbstractTensorMap</code> this has no meaning. However, we can query <code>space(t, i)</code>, the space associated with the <code>i</code>th index. For <code>i ‚àà 1:N‚ÇÅ</code>, this corresponds to <code>codomain(t, i) = codomain(t)[i]</code>. For <code>j = i-N‚ÇÅ ‚àà (1:N‚ÇÇ)</code>, this corresponds to <code>dual(domain(t, j)) = dual(domain(t)[j])</code>.</p><p>The total number of indices, i.e. <code>N‚ÇÅ+N‚ÇÇ</code>, is given by <code>numind(t)</code>, with <code>N‚ÇÅ == numout(t)</code> and <code>N‚ÇÇ == numin(t)</code>, the number of outgoing and incoming indices. There are also the unexported methods <code>TensorKit.codomainind(t)</code> and <code>TensorKit.domainind(t)</code> which return the tuples <code>(1, 2, ‚Ä¶, N‚ÇÅ)</code> and <code>(N‚ÇÅ+1, ‚Ä¶, N‚ÇÅ+N‚ÇÇ)</code>, and are useful for internal purposes. The type parameter <code>S&lt;:ElementarySpace</code> can be obtained as <code>spacetype(t)</code>; the corresponding sector can directly obtained as <code>sectortype(t)</code> and is <code>Trivial</code> when <code>S != RepresentationSpace</code>. The underlying field scalars of <code>S</code> can also directly be obtained as <code>field(t)</code>. This is different from <code>eltype(t)</code>, which returns the type of <code>Number</code> in the tensor data, i.e. the type parameter <code>T</code> in the (subtype of) <code>DenseMatrix{T}</code> in which the matrix blocks are stored. Note that during construction, a (one-time) warning is printed if <code>!(T ‚äÇ field(S))</code>. The specific <code>DenseMatrix{T}</code> subtype in which the tensor data is stored is obtained as <code>storagetype(t)</code>. Each of the methods <code>numind</code>, <code>numout</code>, <code>numin</code>, <code>TensorKit.codomainind</code>, <code>TensorKit.domainind</code>, <code>spacetype</code>, <code>sectortype</code>, <code>field</code>, <code>eltype</code> and <code>storagetype</code> work in the type domain as well, i.e. they are encoded in <code>typeof(t)</code>.</p><p>Finally, there are methods to probe the data, which we already encountered. <code>blocksectors(t)</code> returns an iterator over the different coupled sectors that can be obtained from fusing the uncoupled sectors available in the domain, but they must also be obtained from fusing the uncoupled sectors available in the codomain (i.e. it is the intersection of both <code>blocksectors(codomain(t))</code> and <code>blocksectors(domain(t))</code>). For a specific sector <code>c ‚àà blocksectors(t)</code>, <code>block(t, c)</code> returns the corresponding data. Both are obtained together with <code>blocks(t)</code>, which returns an iterator over the pairs <code>c=&gt;block(t, c)</code>. Furthermore, there is <code>fusiontrees(t)</code> which returns an iterator over splitting-fusion tree pairs <code>(f‚ÇÅ,f‚ÇÇ)</code>, for which the corresponding data is given by <code>t[f‚ÇÅ,f‚ÇÇ]</code> (i.e. using Base.getindex).</p><h2 id="ss_tensor_linalg-1"><a class="docs-heading-anchor" href="#ss_tensor_linalg-1">Vector space and linear algebra operations</a><a class="docs-heading-anchor-permalink" href="#ss_tensor_linalg-1" title="Permalink"></a></h2><p><code>AbstractTensorMap</code> instances <code>t</code> represent linear maps, i.e. homomorphisms in a <code>ùïú</code>-linear category, just like matrices. To a large extent, they follow the interface of <code>Matrix</code> in Julia&#39;s <code>LinearAlgebra</code> standard library. Many methods from <code>LinearAlgebra</code> are (re)exported by TensorKit.jl, and can then us be used without <code>using LinearAlgebra</code> explicitly. In all of the following methods, the implementation acts directly on the underlying matrix blocks (typically using the same method) and never needs to perform any basis transforms.</p><p>In particular, <code>AbstractTensorMap</code> instances can be composed, provided the domain of the first object coincides with the codomain of the second. Composing tensor maps uses the regular multiplication symbol as in <code>t = t1*t2</code>, which is also used for matrix multiplication. TensorKit.jl also supports (and exports) the mutating method <code>mul!(t, t1, t2)</code>. We can then also try to invert a tensor map using <code>inv(t)</code>, though this can only exist if the domain and codomain are isomorphic, which can e.g. be checked as <code>fuse(codomain(t)) == fuse(domain(t))</code>. If the inverse is composed with another tensor <code>t2</code>, we can use the syntax <code>t1\t2</code> or <code>t2/t1</code>. However, this syntax also accepts instances <code>t1</code> whose domain and codomain are not isomorphic, and then amounts to <code>pinv(t1)</code>, the Moore-Penrose pseudoinverse. This, however, is only really justified as minimizing the least squares problem if <code>spacetype(t) &lt;: EuclideanSpace</code>.</p><p><code>AbstractTensorMap</code> instances behave themselves as vectors (i.e. they are <code>ùïú</code>-linear) and so they can be multiplied by scalars and, if they live in the same space, i.e. have the same domain and codomain, they can be added to each other. There is also a <code>zero(t)</code>, the additive identity, which produces a zero tensor with the same domain and codomain as <code>t</code>. In addition, <code>TensorMap</code> supports basic Julia methods such as <code>fill!</code> and <code>copyto!</code>, as well as <code>copy(t)</code> to create a copy with independent data. Aside from basic <code>+</code> and <code>*</code> operations, TensorKit.jl reexports a number of efficient in-place methods from <code>LinearAlgebra</code>, such as <code>axpy!</code> (for <code>y ‚Üê Œ± * x + y</code>), <code>axpby!</code> (for <code>y ‚Üê Œ± * x + Œ≤ * y</code>), <code>lmul!</code> and <code>rmul!</code> (for <code>y ‚Üê Œ±*y</code> and <code>y ‚Üê y*Œ±</code>, which is typically the same) and <code>mul!</code>, which can also be used for out-of-place scalar multiplication <code>y ‚Üê Œ±*x</code>.</p><p>For <code>t::AbstractTensorMap{S}</code> where <code>S&lt;:EuclideanSpace</code>, henceforth referred to as a <code>(Abstract)EuclideanTensorMap</code>, we can compute <code>norm(t)</code>, and for two such instances, the inner product <code>dot(t1, t2)</code>, provided <code>t1</code> and <code>t2</code> have the same domain and codomain. Furthermore, there is <code>normalize(t)</code> and <code>normalize!(t)</code> to return a scaled version of <code>t</code> with unit norm. These operations should also exist for <code>S&lt;:InnerProductSpace</code>, but requires an interface for defining a custom inner product in these spaces. Currently, there is no concrete subtype of <code>InnerProductSpace</code> that is not a subtype of <code>EuclideanSpace</code>. In particular, <code>CartesianSpace</code>, <code>ComplexSpace</code> and <code>RepresentationSpace</code> are all subtypes of <code>EuclideanSpace</code>.</p><p>With instances <code>t::AbstractEuclideanTensorMap</code> there is associated an adjoint operation, given by <code>adjoint(t)</code> or simply <code>t&#39;</code>, such that <code>domain(t&#39;) == codomain(t)</code> and <code>codomain(t&#39;) == domain(t)</code>. Note that for an instance <code>t::TensorMap{S,N‚ÇÅ,N‚ÇÇ}</code>, <code>t&#39;</code> is simply stored in a wrapper called <code>AdjointTensorMap{S,N‚ÇÇ,N‚ÇÅ}</code>, which is another subtype of <code>AbstractTensorMap</code>. This should be mostly unvisible to the user, as all methods should work for this type as well. It can be hard to reason about the index order of <code>t&#39;</code>, i.e. index <code>i</code> of <code>t</code> appears in <code>t&#39;</code> at index position <code>j = TensorKit.adjointtensorindex(t, i)</code>, where the latter method is typically not necessary and hence unexported. There is also a plural <code>TensorKit.adjointtensorindices</code> to convert multiple indices at once. Note that, because the adjoint interchanges domain and codomain, we have <code>space(t&#39;, j) == space(t, i)&#39;</code>.</p><p><code>AbstractTensorMap</code> instances can furthermore be tested for exact (<code>t1 == t2</code>) or approximate (<code>t1 ‚âà t2</code>) equality, though the latter requires <code>norm</code> can be computed.</p><p>When tensor map instances are endomorphisms, i.e. they have the same domain and codomain, there is a multiplicative identity which can be obtained as <code>one(t)</code> or <code>one!(t)</code>, where the latter overwrites the contents of <code>t</code>. Furthermore, we can compute the trace of an endomorphism <code>t</code> via <code>tr(t)</code> and exponentiate it using <code>exp(t)</code>, or if the contents of <code>t</code> can be destroyed in the process, <code>exp!(t)</code>. Furthermore, there are a number of tensor factorizations for both endomorphisms and general homomorphism that we discuss below.</p><p>Finally, there are a number of operations that also belong in this paragraph because of their analogy to common matrix operations. The tensor product of two <code>TensorMap</code> instances <code>t1</code> and <code>t2</code> is obtained as <code>t1 ‚äó t2</code> and results in a new <code>TensorMap</code> with <code>codomain(t1‚äót2) = codomain(t1) ‚äó codomain(t2)</code> and <code>domain(t1‚äót2) = domain(t1) ‚äó domain(t2)</code>. If we have two <code>TensorMap{S,N,1}</code> instances <code>t1</code> and <code>t2</code> with the same codomain, we can combine them in a way that is analoguous to <code>hcat</code>, i.e. we stack them such that the new tensor <code>catdomain(t1, t2)</code> has also the same codomain, but has a domain which is <code>domain(t1) ‚äï domain(t2)</code>. Similarly, if <code>t1</code> and <code>t2</code> are of type <code>TensorMap{S,1,N}</code> and have the same domain, the operation <code>catcodomain(t1, t2)</code> results in a new tensor with the same domain and a codomain given by <code>codomain(t1) ‚äï codomain(t2)</code>, which is the analogy of <code>vcat</code>. Note that direct sum only makes sense between <code>ElementarySpace</code> objects, i.e. there is no way to give a tensor product meaning to a direct sum of tensor product spaces.</p><h2 id="Index-manipulations-1"><a class="docs-heading-anchor" href="#Index-manipulations-1">Index manipulations</a><a class="docs-heading-anchor-permalink" href="#Index-manipulations-1" title="Permalink"></a></h2><p>In many cases, the bipartition of tensor indices (i.e. <code>ElementarySpace</code> instances) between the codomain and domain is not fixed throughout the different operations that need to be performed on that tensor map, i.e. we want to use the duality to move spaces from domain to codomain and vice versa. Furthermore, we want to use the braiding to reshuffle the order of the indices.</p><p>For this, we use an interface that is closely related to that for manipulating splitting- fusion tree pairs, namely <code>braid</code> and <code>permute</code>, with the interface</p><p><code>braid(t::AbstractTensorMap{S,N‚ÇÅ,N‚ÇÇ}, levels::NTuple{N‚ÇÅ+N‚ÇÇ,Int}, p1::NTuple{N‚ÇÅ‚Ä≤,Int}, p2::NTuple{N‚ÇÇ‚Ä≤,Int}) -&gt; AbstractTensorMap{S,N‚ÇÅ‚Ä≤,N‚ÇÇ‚Ä≤}</code></p><p>and</p><p><code>permute(t::AbstractTensorMap{S,N‚ÇÅ,N‚ÇÇ}, p1::NTuple{N‚ÇÅ‚Ä≤,Int}, p2::NTuple{N‚ÇÇ‚Ä≤,Int}; copy = true) -&gt; AbstractTensorMap{S,N‚ÇÅ‚Ä≤,N‚ÇÇ‚Ä≤}</code></p><p>In these methods, <code>p1</code> and <code>p2</code> specify which of the original tensor indices ranging from <code>1</code> to <code>N‚ÇÅ+N‚ÇÇ</code> make up the new codomain (with <code>N‚ÇÅ‚Ä≤</code> spaces) and new domain (with <code>N‚ÇÇ‚Ä≤</code> spaces). Hence, <code>(p1..., p2...)</code> should be a valid permutation of <code>1:(N‚ÇÅ+N‚ÇÇ)</code>. Note that, throughout TensorKit.jl, permutations are always specified using tuples of <code>Int</code>s, for reasons of type stability. For <code>braid</code>, we also need to specify <code>levels</code> or depths for each of the indices of the original tensor, which determine whether indices will braid over or underneath each other (use the braiding or its inverse). We refer to the section on <a href="../sectors/#ss_fusiontrees-1">manipulating fusion trees</a> for more details.</p><p>When <code>BraidingStyle(sectortype(t)) isa SymmetricBraiding</code>, we can use the simpler interface of <code>permute</code>, which does not require the argument <code>levels</code>. <code>permute</code> accepts a keyword argument <code>copy</code>. When it takes the default value <code>true</code>, the result will be a tensor with newly allocated data that can independently be modified from that of the input tensor <code>t</code>. When <code>copy=false</code>, <code>permute</code> can try to return the result in a way that it shares its data with the input tensor <code>t</code>, though this is only possible in specific cases (e.g. when <code>sectortype(S) == Trivial</code> and <code>(p1..., p2...) = (1:(N‚ÇÅ+N‚ÇÇ)...)</code>).</p><p>Both <code>braid</code> and <code>permute</code> come in a version where the result is stored in an already existing tensor, i.e. <code>braid!(tdst, tsrc, levels, p1, p2)</code> and <code>permute!(tdst, tsrc, p1, p2)</code>.</p><p>Another operation that belongs und index manipulations is taking the <code>transpose</code> of a tensor, i.e. <code>LinearAlgebra.transpose(t)</code> and <code>LinearAlgebra.transpose!(tdst, tsrc)</code>, both of which are reexported by TensorKit.jl. Note that <code>transpose(t)</code> is not simply equal to reshuffling domain and codomain with <code>braid(t, (1:(N‚ÇÅ+N‚ÇÇ)...), reverse(domainind(tsrc)), reverse(codomainind(tsrc))))</code>. Indeed, the graphical representation (where we draw the codomain and domain as a single object), makes clear that this introduces an additional (inverse) twist, which is then compensated in the <code>transpose</code> implementation.</p><p><img src="../img/tensor-transpose.svg" alt="transpose"/></p><p>In categorical language, the reason for this extra twist is that we use the left coevaluation <code>Œ∑</code>, but the right evaluation <code>\tilde{œµ}</code>, when repartitioning the indices between domain and codomain.</p><p>There are a number of other index related manipulations. We can apply a twist (or inverse twist) to one of the tensor map indices via <code>twist(t, i; inv = false)</code> or <code>twist!(t, i; inv = false)</code>. Note that the latter method does not store the result in a new destination tensor, but just modifies the tensor <code>t</code> in place. Twisting several indices simultaneously can be obtained by using the defining property</p><p><span>$Œ∏_{V‚äóW} = œÑ_{W,V} ‚àò (Œ∏_W ‚äó Œ∏_V) ‚àò œÑ_{V,W} = (Œ∏_V ‚äó Œ∏_W) ‚àò œÑ_{W,V} ‚àò œÑ_{V,W}.$</span></p><p>but is currently not implemented explicitly.</p><p>Another operation that one might be expecting is to fuse or join indices, and its inverse, to split a given index into two or more indices. For a plain tensor (i.e. with <code>sectortype(t) == Trivial</code>) amount to the equivalent of <code>reshape</code> on the multidimensional data. However, this represents only one possibility, as there is no canonically unique way to embed the tensor product of two spaces <code>V‚ÇÅ ‚äó V‚ÇÇ</code> in a new space <code>V = fuse(V‚ÇÅ‚äóV‚ÇÇ)</code>. Such a mapping can always be accompagnied by a basis transform. So far, we do not provide a dedicated interface to e.g. fuse two neighboring indices <code>i</code> and <code>i+1</code> into a single index, but this can easily be accomplished by contracting the tensor (see below) with a dedicated tensor <code>x = TensorMap(randisometry, fuse(V‚ÇÅ‚äóV‚ÇÇ), V‚ÇÅ ‚äó V‚ÇÇ)</code> and its inverse <code>x&#39;</code>. Instead of <code>randisometry</code>, one can also use <code>x = isomorphism(fuse(V‚ÇÅ‚äóV‚ÇÇ), V‚ÇÅ ‚äó V‚ÇÇ)</code> or <code>x = unitary(fuse(V‚ÇÅ‚äóV‚ÇÇ), V‚ÇÅ ‚äó V‚ÇÇ)</code>.</p><p>Note that a typical algorithms is not expected to often need to fuse and split indices (to continue...)</p><h2 id="Tensor-factorizations-1"><a class="docs-heading-anchor" href="#Tensor-factorizations-1">Tensor factorizations</a><a class="docs-heading-anchor-permalink" href="#Tensor-factorizations-1" title="Permalink"></a></h2><p>As tensors are linear maps, they have various kinds of factorizations. Endomorphism, i.e. tensor maps <code>t</code> with <code>codomain(t) == domain(t)</code>, have an eigenvalue decomposition. For this, we overload both <code>LinearAlgebra.eigen(t; kwargs...)</code> and <code>LinearAlgebra.eigen!(t; kwargs...)</code>, where the latter destroys <code>t</code> in the process. The keyword arguments are the same that are accepted by <code>LinearAlgebra.eigen(!)</code> for matrices. The result is returned as <code>D, V = eigen(t)</code>, such that <code>t*V ‚âà V*D</code>. For given <code>t::TensorMap{S,N,N}</code>, <code>V</code> is a <code>TensorMap{S,N,1}</code>, whose codomain corresponds to that of <code>t</code>, but whose domain is a single space <code>S</code> (or more correctly a <code>ProductSpace{S,1}</code>), that corresponds to <code>fuse(codomain(t))</code>. The eigenvalues are encoded in <code>D</code>, a <code>TensorMap{S,1,1}</code>, whose domain and codomain correspond to the domain of <code>V</code>. Indeed, we cannot reasonably associate a tensor product structure with the different eigenvalues. Note that <code>D</code> stores the eigenvalues on the diagonal of a (collection of) <code>DenseMatrix</code> instance(s), as there is currently no dedicated <code>DiagonalTensorMap</code> or diagonal storage support.</p><p>We also define <code>LinearAlgebra.ishermitian(t)</code>, which can only return true for instances of <code>AbstractEuclideanTensorMap</code>. In all other cases, as the inner product is not defined, there is no notion of hermiticity (i.e. we are not working in a <code>‚Ä†</code>-category). For instances of <code>EuclideanTensorMap</code>, we also define and export the routines <code>eigh</code> and <code>eigh!</code>, which compute the eigenvalue decomposition under the guarantee (not checked) that the map is hermitian. Hence, eigenvalues will be real and <code>V</code> will be unitary with <code>eltype(V) == eltype(t)</code>. We also define and export <code>eig</code> and <code>eig!</code>, which similarly assume that the <code>TensorMap</code> is not hermitian (hence this does not require <code>EuclideanTensorMap</code>), and always returns complex values eigenvalues and eigenvectors. Like for matrices, <code>LinearAlgebra.eigen</code> is type unstable and checks hermiticity at run-time, then falling back to either <code>eig</code> or <code>eigh</code>.</p><p>Other factorizations that are provided by TensorKit.jl are orthogonal or unitary in nature, and thus always require a <code>AbstractEuclideanTensorMap</code>. However, they don&#39;t require equal domain and codomain. Let us first discuss the <em>singular value decomposition</em>, for which we define and export the methods <code>tsvd</code> and <code>tsvd!</code> (where as always, the latter destroys the input).</p><p><code>U, Œ£, V ∞, œµ = tsvd(t; truncation = notrunc(), p::Real = 2, alg::OrthogonalFactorizationAlgorithm = SDD())</code></p><p>This computes a (possibly truncated) singular value decomposition of <code>t::TensorMap{S,N‚ÇÅ,N‚ÇÇ}</code> (with <code>S&lt;:EuclideanSpace</code>), such that <code>norm(t - U*Œ£*V ∞) ‚âà œµ</code>, where <code>U::TensorMap{S,N‚ÇÅ,1}</code>, <code>S::TensorMap{S,1,1}</code>, <code>V ∞::TensorMap{S,1,N‚ÇÇ}</code> and <code>œµ::Real</code>. <code>U</code> is an isometry, i.e. <code>U&#39;*U</code> approximates the identity, whereas <code>U*U&#39;</code> is an idempotent (squares to itself). The same holds for <code>adjoint(V ∞)</code>. The domain of <code>U</code> equals the domain and codomain of <code>Œ£</code> and the codomain of <code>V ∞</code>. In the case of <code>truncation = notrunc()</code> (default value, see below), this space is given by <code>min(fuse(codomain(t)), fuse(domain(t)))</code>. The singular values are contained in <code>Œ£</code> and are stored on the diagonal of a (collection of) <code>DenseMatrix</code> instance(s), similar to the eigenvalues before.</p><p>The keyword argument <code>truncation</code> provides a way to control the truncation, and is connected to the keyword argument <code>p</code>. The default value <code>notrunc()</code> implies no truncation, and thus <code>œµ = 0</code>. Other valid options are</p><ul><li><p><code>truncerr(Œ∑::Real)</code>: truncates such that the <code>p</code>-norm of the truncated singular values   is smaller than <code>Œ∑</code> times the <code>p</code>-norm of all singular values;</p></li><li><p><code>truncdim(œá::Integer)</code>: finds the optimal truncation such that the equivalent total   dimension of the internal vector space is no larger than <code>œá</code>;</p></li><li><p><code>truncspace(W)</code>: truncates such that the dimension of the internal vector space is   smaller than that of <code>W</code> in any sector, i.e. with   <code>W‚ÇÄ = min(fuse(codomain(t)), fuse(domain(t)))</code> this option will result in   <code>domain(U) == domain(Œ£) == codomain(Œ£) == codomain(V·µà) == min(W, W‚ÇÄ)</code>;</p></li><li><p><code>trunbelow(Œ∑::Real)</code>: truncates such that every singular value is larger then <code>Œ∑</code>; this   is different from <code>truncerr(Œ∑)</code> with <code>p = Inf</code> because it works in absolute rather than   relative values.</p></li></ul><p>Furthermore, the <code>alg</code> keyword can be either <code>SVD()</code> or <code>SDD()</code> (default), which corresponds to two different algorithms in LAPACK to compute singular value decompositions. The default value <code>SDD()</code> uses a divide-and-conquer algorithm and is typically the fastest, but can loose some accuracy. The <code>SVD()</code> method uses a QR-iteration scheme and can be more accurate, but is typically slower. Since Julia 1.3, these two algorithms are also available in the <code>LinearAlgebra</code> standard library, where they are specified as <code>LinearAlgebra.DivideAndConquer()</code> and <code>LinearAlgebra.QRIteration()</code>.</p><p>Note that we defined the new method <code>tsvd</code> (truncated or tensor singular value decomposition), rather than overloading <code>LinearAlgebra.svd</code>. We (will) also support <code>LinearAlgebra.svd(t)</code> as alternative for <code>tsvd(t; truncation = notrunc())</code>, but note that the return values are then given by <code>U, Œ£, V = svd(t)</code> with <code>V = adjoint(V ∞)</code>.</p><p>We also define the following pair of orthogonal factorization algorithms, which are useful when one is not interested in truncating a tensor or knowing the singular values, but only in its image or coimage.</p><ul><li><p><code>Q, R = leftorth(t; alg::OrthogonalFactorizationAlgorithm = QRpos(), kwargs...)</code>:   this produces an isometry <code>Q::TensorMap{S,N‚ÇÅ,1}</code> (i.e. <code>Q&#39;*Q</code> approximates the identity,   <code>Q*Q&#39;</code> is an idempotent, i.e. squares to itself) and a general tensor map   <code>R::TensorMap{1,N‚ÇÇ}</code>, such that <code>t ‚âà Q*R</code>. Here, the domain of <code>Q</code> and thus codomain of   <code>R</code> is a single vector space of type <code>S</code> that is typically given by   <code>min(fuse(codomain(t)), fuse(domain(t)))</code>.</p><p>The underlying algorithm used to compute this decomposition can be chosen among <code>QR()</code>,   <code>QRpos()</code>, <code>QL()</code>, <code>QLpos()</code>, <code>SVD()</code>, <code>SDD()</code>, <code>Polar()</code>. <code>QR()</code> uses the underlying   <code>qr</code> decomposition from <code>LinearAlgebra</code>, while <code>QRpos()</code> (the default) adds a correction   to that to make sure that the diagonal elements of <code>R</code> are positive. Both result in   block matrices in <code>R</code> which are upper triangular. <code>QL()</code> and <code>QLpos()</code> similarly result   in a lower triangular block matrices in <code>R</code>, but only work if all block matrices are   tall, i.e. <code>blockdim(codomain(t), c) &gt;= blockdim(domain(t), c)</code> for all <code>c ‚àà   blocksectors(t)</code>. All of these methods assume <code>t</code> has full rank.</p><p>If this is not the case, one can also use <code>alg = SVD()</code> or <code>alg = SDD()</code>, with extra   keywords to control the absolute (<code>atol</code>) or relative (<code>rtol</code>) tolerance. We then set   <code>Q=U</code> and <code>R=Œ£*V ∞</code> from the corresponding singular value decomposition, where only   these singular values <code>œÉ &gt;= max(atol, norm(t)*rtol)</code> (and corresponding singular vectors   in <code>U</code>) are kept. More finegrained control on the chosen singular values can be   obtained with <code>tsvd</code> and its <code>truncation</code> keyword.</p><p>Finally, <code>Polar()</code> sets <code>Q=U*V ∞</code> and <code>R = (V ∞)&#39;*Œ£*V ∞</code>, such that <code>R</code> is positive   definite; in this case <code>SDD()</code> is used to actually compute the singular value   decomposition and no <code>atol</code> or <code>rtol</code> can be provided.</p></li><li><p><code>L, Q = leftorth(t; alg::OrthogonalFactorizationAlgorithm = QRpos())</code>: this produces a   general tensor map <code>L::TensorMap{S,N‚ÇÅ,1}</code> and the adjoint of an isometry   <code>Q::TensorMap{S,1,N‚ÇÇ}</code>, such that <code>t ‚âà L*Q</code>. Here, the domain of <code>L</code> and thus codomain   of <code>Q</code> is a single vector space of type <code>S</code> that is typically given by   <code>min(fuse(codomain(t)), fuse(domain(t)))</code>.</p><p>The underlying algorithm used to compute this decomposition can be chosen among <code>LQ()</code>,   <code>LQpos()</code>, <code>RQ()</code>, <code>RQpos()</code>, <code>SVD()</code>, <code>SDD()</code>, <code>Polar()</code>. <code>LQ()</code> uses the underlying   <code>qr</code> decomposition from <code>LinearAlgebra</code> on the transposed data, and leads to lower   triangular block matrices in <code>L</code>; <code>LQpos()</code> makes sure the diagonal elements are   positive. <code>RQ()</code> and <code>RQpos()</code> similarly result in upper triangular block matrices in   <code>L</code>, but only works for wide matrices, i.e. <code>blockdim(codomain(t), c) &lt;=   blockdim(domain(t), c)</code> for all <code>c ‚àà blocksectors(t)</code>. All of these methods assume <code>t</code>   has full rank.</p><p>If this is not the case, one can also use <code>alg = SVD()</code> or <code>alg = SDD()</code>, with extra   keywords to control the absolute (<code>atol</code>) or relative (<code>rtol</code>) tolerance. We then set   <code>L=U*Œ£</code> and <code>Q=V ∞</code> from the corresponding singular value decomposition, where only these   singular values <code>œÉ &gt;= max(atol, norm(t)*rtol)</code> (and corresponding singular vectors in   <code>V ∞</code>) are kept. More finegrained control on the chosen singular values can be obtained   with <code>tsvd</code> and its <code>truncation</code> keyword.</p><p>Finally, <code>Polar()</code> sets <code>L = U*Œ£*U&#39;</code> and <code>Q=U*V ∞</code>, such that <code>L</code> is positive definite;   in this case <code>SDD()</code> is used to actually compute the singular value decomposition and no   <code>atol</code> or <code>rtol</code> can be provided.</p></li></ul><p>Furthermore, we can compute an orthonormal basis for the orthogonal complement of the image and of the co-image (i.e. the kernel) with the following methods:</p><ul><li><p><code>N = leftnull(t; alg::OrthogonalFactorizationAlgorithm = QR(), kwargs...)</code>:   returns an isometric <code>TensorMap{S,N‚ÇÅ,1}</code> (i.e. <code>N&#39;*N</code> approximates the identity) such   that <code>N&#39;*t</code> is approximately zero.</p><p>Here, <code>alg</code> can be <code>QR()</code> (<code>QRpos()</code> acts identically in this case), which assumes that   <code>t</code> is full rank in all of its blocks and only returns an orthonormal basis for the   missing columns.</p><p>If this is not the case, one can also use <code>alg = SVD()</code> or <code>alg = SDD()</code>, with extra   keywords to control the absolute (<code>atol</code>) or relative (<code>rtol</code>) tolerance. We then   construct <code>N</code> from the left singular vectors corresponding to singular values   <code>œÉ &lt; max(atol, norm(t)*rtol)</code>.</p></li><li><p><code>N = rightnull(t; alg::OrthogonalFactorizationAlgorithm = QR(), kwargs...)</code>:   returns a <code>TensorMap{S,1,N‚ÇÇ}</code> with isometric adjoint (i.e. <code>N*N&#39;</code> approximates the   identity) such that <code>t*N&#39;</code> is approximately zero.</p><p>Here, <code>alg</code> can be <code>LQ()</code> (<code>LQpos()</code> acts identically in this case), which assumes that   <code>t</code> is full rank in all of its blocks and only returns an orthonormal basis for the   missing rows.</p><p>If this is not the case, one can also use <code>alg = SVD()</code> or <code>alg = SDD()</code>, with extra   keywords to control the absolute (<code>atol</code>) or relative (<code>rtol</code>) tolerance. We then   construct <code>N</code> from the right singular vectors corresponding to singular values   <code>œÉ &lt; max(atol, norm(t)*rtol)</code>.</p></li></ul><p>Note that the methods <code>leftorth</code>, <code>rightorth</code>, <code>leftnull</code> and <code>rightnull</code> also come in a form with exclamation mark, i.e. <code>leftorth!</code>, <code>rightorth!</code>, <code>leftnull!</code> and <code>rightnull!</code>, which destroy the input tensor <code>t</code>.</p><p>Finally, note that each of the factorizations take a single argument, the tensor map <code>t</code>, and a number of keyword arguments. They perform the factorization according to the given codomain and domain of the tensor map. In many cases, we want to perform the factorization according to a different bipartition of the indices. When <code>BraidingStyle(sectortype(t)) isa SymmetricBraiding</code>, we can immediately specify an alternative bipartition of the indices of <code>t</code> in all of these methods, in the form</p><pre><code class="language-none">factorize(t::AbstracTensorMap, pleft::NTuple{N‚ÇÅ‚Ä≤,Int}, pright::NTuple{N‚ÇÇ‚Ä≤,Int}; kwargs...)</code></pre><p>where <code>pleft</code> will be the indices in the codomain of the new tensor map, and <code>pright</code> the indices of the domain. Here, <code>factorize</code> is any of the methods <code>LinearAlgebra.eigen</code>, <code>eig</code>, <code>eigh</code>, <code>tsvd</code>, <code>LinearAlgebra.svd</code>, <code>leftorth</code>, <code>rightorth</code>, <code>leftnull</code> and <code>rightnull</code>. This signature does not allow for the exclamation mark, because it amounts to</p><pre><code class="language-none">factorize!(permute(t, pleft, pright; copy = true); kwargs...)</code></pre><p>where <code>permute</code> was introduced and discussed in the previous section. When the braiding is not symmetric, the user should manually apply <code>braid</code> to bring the tensor map in proper form before performing the factorization.</p><h2 id="Bosonic-tensor-contractions-and-tensor-networks-1"><a class="docs-heading-anchor" href="#Bosonic-tensor-contractions-and-tensor-networks-1">Bosonic tensor contractions and tensor networks</a><a class="docs-heading-anchor-permalink" href="#Bosonic-tensor-contractions-and-tensor-networks-1" title="Permalink"></a></h2><p>TODO</p><h2 id="Fermionic-tensor-contractions-1"><a class="docs-heading-anchor" href="#Fermionic-tensor-contractions-1">Fermionic tensor contractions</a><a class="docs-heading-anchor-permalink" href="#Fermionic-tensor-contractions-1" title="Permalink"></a></h2><p>TODO</p><h2 id="Anyonic-tensor-contractions-1"><a class="docs-heading-anchor" href="#Anyonic-tensor-contractions-1">Anyonic tensor contractions</a><a class="docs-heading-anchor-permalink" href="#Anyonic-tensor-contractions-1" title="Permalink"></a></h2><p>TODO</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../sectors/">¬´ Sectors, representation spaces and fusion trees</a><a class="docs-footer-nextpage" href="../../lib/spaces/">Vector spaces, symmetry sectors an fusion trees ¬ª</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 5 February 2020 17:18">Wednesday 5 February 2020</span>. Using Julia version 1.0.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
